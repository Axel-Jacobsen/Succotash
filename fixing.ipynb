{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Philosophy of this network:\n",
    "    The goal that I had while writing this was for me to cement my understanding of the basic fully connected feed-forward network.\n",
    "    My original implementation was quite slow, as it was not taking advantage of numpy vectorization - this version does. You can compare\n",
    "    the previous version of this file to this one (filename nn.py, commit 9cb3da3ce582e940ed862f95930879c8be1721d1), and see the \n",
    "    significant training speed differences. I will say, adding vectorization makes the code less readable (and also increases the\n",
    "    required memory) as I had to pad all vectors and matricies with zeros so each set of data had the same shape (and therefore it could be vectorized).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from loss_fcns import squared_loss, cross_entropy_loss\n",
    "from activations import eLU, ReLU, leaky_ReLU, sigmoid, linear, tanh, softmax\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "def greater_than_9():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    return (\n",
    "        np.load(\"fake_data/X_train.npy\"),\n",
    "        _get_one_hot(np.load(\"fake_data/Y_train.npy\"), 2),\n",
    "        np.load(\"fake_data/X_test.npy\"),\n",
    "        _get_one_hot(np.load(\"fake_data/Y_test.npy\"), 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def mnist():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    def load_data(fname):\n",
    "        data_folder = \"mnist_data/\"\n",
    "        with open(data_folder + fname, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    x_train = load_data(\"train-images-idx3-ubyte\")\n",
    "    y_train = load_data(\"train-labels-idx1-ubyte\")\n",
    "    x_test = load_data(\"t10k-images-idx3-ubyte\")\n",
    "    y_test = load_data(\"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "    return (\n",
    "        x_train[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_train[8:], 10).reshape((10, -1)),\n",
    "        x_test[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_test[8:], 10).reshape((10, -1)),\n",
    "    )\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = greater_than_9()\n",
    "print(\"data loaded\")\n",
    "\n",
    "# net = ffnn.FFNN([784, 512, 128, 10], [tanh, tanh, softmax], cross_entropy_loss)\n",
    "layers = [2, 2, 2]\n",
    "hs = [ReLU, softmax]\n",
    "cost_fcn = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(random=True):\n",
    "    \"\"\"\n",
    "    random == False for generating empty weight/bias matricies\n",
    "    \"\"\"\n",
    "    layer_arr = layers\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    layer_iter = iter(layer_arr)\n",
    "    prev_dim = layer_iter.__next__()\n",
    "\n",
    "    for i, dim in enumerate(layer_iter):\n",
    "        if random:\n",
    "            bound = np.sqrt(2 / layer_arr[i])\n",
    "            weight_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, prev_dim)).astype(np.float32)\n",
    "            biases_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, 1)).astype(np.float32)\n",
    "        else:\n",
    "            weight_matrix = np.zeros((dim, prev_dim), dtype=np.float32)\n",
    "            biases_matrix = np.zeros((dim, 1), dtype=np.float32)\n",
    "\n",
    "        weights.append(weight_matrix)\n",
    "        biases.append(biases_matrix)\n",
    "        prev_dim = dim\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = make_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) 1.0\n",
      "(2, 10) 10.0\n",
      "(2, 10) 10.000001\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(xs, training=False):\n",
    "    \"\"\"\n",
    "    Feed-forward through the network, saving the activations and non-linearities\n",
    "    after each layer for backprop.\n",
    "\n",
    "    xs has to be of shape (num features, batch_size)\n",
    "    - x, z, a are all vectors of inputs, outputs, and linear outputs at layers\n",
    "    \"\"\"\n",
    "    zs = []\n",
    "    activations = []\n",
    "\n",
    "    activation = xs.astype(np.float32) / 255\n",
    "    activations.append(activation)\n",
    "    for i, (W, b) in enumerate(zip(weights, biases)):\n",
    "        z = np.einsum(\"ij, jb -> ib\", W, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = hs[i].f(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "    return (activation, activations, zs) if training else activation\n",
    "\n",
    "d1 = np.asarray([[1],[1]])\n",
    "print(d1.shape, feed_forward(d1).sum())\n",
    "\n",
    "d2 = np.asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],[9,8,7,6,5,4,3,2,1,0]])\n",
    "print(d2.shape, feed_forward(d2).sum())\n",
    "\n",
    "print(X_train[:,:10].shape, feed_forward(X_train[:,:10]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9],\n",
       "        [3]]),\n",
       " array([[0.38942432],\n",
       "        [0.6105757 ]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def back_prop(xs, ts):\n",
    "    \"\"\"\n",
    "    xs,ts are lists of vectors (ts are targets for training i.e. true output given input x)\n",
    "    \"\"\"\n",
    "    weight_grads, bias_grads = make_network(random=False)\n",
    "    ys, activations, zs = feed_forward(xs, training=True)\n",
    "\n",
    "    # delta_L = grad cost_fcn(outputs) * activation_fcn.deriv(weighted_output_last_layer)\n",
    "    # should be hadamard product\n",
    "    # Also, ys is just activations[-1]\n",
    "    assert ts.shape == ys.shape\n",
    "\n",
    "    delta = cost_fcn.deriv(ts, ys) * hs[-1].deriv(zs[-1])\n",
    "    batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-2])\n",
    "\n",
    "    # sum along batch\n",
    "    bias_grads[-1][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "    weight_grads[-1][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    # back propogate through layers\n",
    "    for l in range(2, len(layers)):\n",
    "        nonlinear_deriv = hs[-l].deriv(zs[-l])\n",
    "        delta = np.dot(weights[-l + 1].T, delta) * nonlinear_deriv\n",
    "        batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-l - 1])\n",
    "\n",
    "        bias_grads[-l][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "        weight_grads[-l][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    for new_b, new_g, self_b, self_g in zip(bias_grads, weight_grads, biases, weights):\n",
    "        assert new_b.shape == self_b.shape\n",
    "        assert new_g.shape == self_g.shape\n",
    "\n",
    "    return weight_grads, bias_grads\n",
    "\n",
    "m = 1\n",
    "n = 10\n",
    "back_prop(X_train[:,m:m+n], Y_train[:,m:m+n]), X_train[:,m:m+n], Y_train[:,m:m+n]\n",
    "X_train[:,m:m+1], feed_forward(X_train[:,m:m+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(batch_xs, batch_ys, lr):\n",
    "    \"\"\"\n",
    "    batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "    \"\"\"\n",
    "    global weights, biases\n",
    "    weight_grads, bias_grads = back_prop(batch_xs, batch_ys)\n",
    "\n",
    "    weights = [\n",
    "        w - lr * weight_grad for w, weight_grad in zip(weights, weight_grads)\n",
    "    ]\n",
    "    biases = [b - lr * bias_grad for b, bias_grad in zip(biases, bias_grads)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(xs, ys, epochs, batch_size, lr):\n",
    "    \"\"\"\n",
    "    xs/ys is the input/output data, epochs is\n",
    "    the number of batches to train, batch size is the number of input/outputs to\n",
    "    use in each batch, and lr is learning rate.\n",
    "\n",
    "    xs, ys have the input vectors as COLUMNS, so xs shape should be (num_features, batch_size)\n",
    "    e.g. with MNIST, each image is 28*28=784 features, so xs is (784, 60000)\n",
    "    since there are 10 classes in mnist, y should be (10, 60000)\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    # get random indicies from batch\n",
    "    random_indicies = np.random.choice(xs.shape[1], size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        mini_batch(xs[:, random_indicies], ys[:, random_indicies], lr)\n",
    "\n",
    "        ts = ys[:, random_indicies]\n",
    "        ys_out_test = feed_forward(xs[:, random_indicies])\n",
    "\n",
    "        loss = cost_fcn.f(ts, ys_out_test)\n",
    "        train_loss = np.mean(loss)\n",
    "\n",
    "        losses.append(train_loss)\n",
    "        accuracies.append(np.sum(np.argmax(ts, axis=0) == np.argmax(ys_out_test, axis=0)) / ts.shape[-1])\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"epoch {epoch} \\t train loss {train_loss:.8f}\")\n",
    "\n",
    "    return losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t train loss 0.71116486\n",
      "epoch 50 \t train loss 0.68349109\n",
      "epoch 100 \t train loss 0.68249348\n",
      "epoch 150 \t train loss 0.68138074\n",
      "epoch 200 \t train loss 0.68012586\n",
      "epoch 250 \t train loss 0.67869874\n",
      "epoch 300 \t train loss 0.67706579\n",
      "epoch 350 \t train loss 0.67518935\n",
      "epoch 400 \t train loss 0.67302738\n",
      "epoch 450 \t train loss 0.67053290\n",
      "epoch 500 \t train loss 0.66765387\n",
      "epoch 550 \t train loss 0.66433304\n",
      "epoch 600 \t train loss 0.66050811\n",
      "epoch 650 \t train loss 0.65611235\n",
      "epoch 700 \t train loss 0.65107563\n",
      "epoch 750 \t train loss 0.64532606\n",
      "epoch 800 \t train loss 0.63879273\n",
      "epoch 850 \t train loss 0.63140896\n",
      "epoch 900 \t train loss 0.62311698\n",
      "epoch 950 \t train loss 0.61387332\n",
      "epoch 1000 \t train loss 0.60365493\n",
      "epoch 1050 \t train loss 0.77176999\n",
      "epoch 1100 \t train loss 0.63693956\n",
      "epoch 1150 \t train loss 0.63305586\n",
      "epoch 1200 \t train loss 0.62960710\n",
      "epoch 1250 \t train loss 0.62634727\n",
      "epoch 1300 \t train loss 0.62317843\n",
      "epoch 1350 \t train loss 0.62006210\n",
      "epoch 1400 \t train loss 0.61698246\n",
      "epoch 1450 \t train loss 0.61371638\n",
      "epoch 1500 \t train loss 0.61049321\n",
      "epoch 1550 \t train loss 0.60728478\n",
      "epoch 1600 \t train loss 0.60429477\n",
      "epoch 1650 \t train loss 0.60108115\n",
      "epoch 1700 \t train loss 0.60127039\n",
      "epoch 1750 \t train loss 0.59813452\n",
      "epoch 1800 \t train loss 0.59199844\n",
      "epoch 1850 \t train loss 0.59195819\n",
      "epoch 1900 \t train loss 0.58611463\n",
      "epoch 1950 \t train loss 0.58319140\n",
      "epoch 2000 \t train loss 0.58290549\n",
      "epoch 2050 \t train loss 0.57994894\n",
      "epoch 2100 \t train loss 0.57703421\n",
      "epoch 2150 \t train loss 0.57423419\n",
      "epoch 2200 \t train loss 0.57149079\n",
      "epoch 2250 \t train loss 0.56877899\n",
      "epoch 2300 \t train loss 0.56593795\n",
      "epoch 2350 \t train loss 0.56292417\n",
      "epoch 2400 \t train loss 0.56001977\n",
      "epoch 2450 \t train loss 0.56013100\n",
      "epoch 2500 \t train loss 0.55723328\n",
      "epoch 2550 \t train loss 0.55424638\n",
      "epoch 2600 \t train loss 0.55139104\n",
      "epoch 2650 \t train loss 0.54600091\n",
      "epoch 2700 \t train loss 0.54572910\n",
      "epoch 2750 \t train loss 0.54299308\n",
      "epoch 2800 \t train loss 0.54036563\n",
      "epoch 2850 \t train loss 0.53779296\n",
      "epoch 2900 \t train loss 0.53525010\n",
      "epoch 2950 \t train loss 0.53272538\n",
      "epoch 3000 \t train loss 0.53133866\n",
      "epoch 3050 \t train loss 0.52835095\n",
      "epoch 3100 \t train loss 0.52558316\n",
      "epoch 3150 \t train loss 0.52297215\n",
      "epoch 3200 \t train loss 0.52044141\n",
      "epoch 3250 \t train loss 0.51795352\n",
      "epoch 3300 \t train loss 0.51549034\n",
      "epoch 3350 \t train loss 0.51304227\n",
      "epoch 3400 \t train loss 0.51060562\n",
      "epoch 3450 \t train loss 0.50817772\n",
      "epoch 3500 \t train loss 0.50575778\n",
      "epoch 3550 \t train loss 0.50334517\n",
      "epoch 3600 \t train loss 0.50093981\n",
      "epoch 3650 \t train loss 0.49854159\n",
      "epoch 3700 \t train loss 0.49615048\n",
      "epoch 3750 \t train loss 0.49445839\n",
      "epoch 3800 \t train loss 0.49178699\n",
      "epoch 3850 \t train loss 0.48926517\n",
      "epoch 3900 \t train loss 0.48682754\n",
      "epoch 3950 \t train loss 0.48443859\n",
      "epoch 4000 \t train loss 0.48237034\n",
      "epoch 4050 \t train loss 0.47991225\n",
      "epoch 4100 \t train loss 0.47752393\n",
      "epoch 4150 \t train loss 0.47517799\n",
      "epoch 4200 \t train loss 0.47285916\n",
      "epoch 4250 \t train loss 0.47055941\n",
      "epoch 4300 \t train loss 0.46827372\n",
      "epoch 4350 \t train loss 0.46599997\n",
      "epoch 4400 \t train loss 0.46373680\n",
      "epoch 4450 \t train loss 0.46148379\n",
      "epoch 4500 \t train loss 0.45924052\n",
      "epoch 4550 \t train loss 0.45700707\n",
      "epoch 4600 \t train loss 0.45478356\n",
      "epoch 4650 \t train loss 0.45257018\n",
      "epoch 4700 \t train loss 0.45036682\n",
      "epoch 4750 \t train loss 0.44817394\n",
      "epoch 4800 \t train loss 0.44599154\n",
      "epoch 4850 \t train loss 0.44381990\n",
      "epoch 4900 \t train loss 0.44165903\n",
      "epoch 4950 \t train loss 0.43950927\n",
      "epoch 5000 \t train loss 0.43737070\n",
      "epoch 5050 \t train loss 0.43524391\n",
      "epoch 5100 \t train loss 0.43367647\n",
      "epoch 5150 \t train loss 0.43138377\n",
      "epoch 5200 \t train loss 0.42917573\n",
      "epoch 5250 \t train loss 0.42702522\n",
      "epoch 5300 \t train loss 0.42491560\n",
      "epoch 5350 \t train loss 0.42283608\n",
      "epoch 5400 \t train loss 0.42078020\n",
      "epoch 5450 \t train loss 0.41874412\n",
      "epoch 5500 \t train loss 0.41672540\n",
      "epoch 5550 \t train loss 0.41472219\n",
      "epoch 5600 \t train loss 0.41273350\n",
      "epoch 5650 \t train loss 0.41075866\n",
      "epoch 5700 \t train loss 0.40879770\n",
      "epoch 5750 \t train loss 0.40726746\n",
      "epoch 5800 \t train loss 0.40520643\n",
      "epoch 5850 \t train loss 0.40320218\n",
      "epoch 5900 \t train loss 0.40124024\n",
      "epoch 5950 \t train loss 0.39931092\n",
      "epoch 6000 \t train loss 0.39740754\n",
      "epoch 6050 \t train loss 0.39552630\n",
      "epoch 6100 \t train loss 0.39366406\n",
      "epoch 6150 \t train loss 0.39181882\n",
      "epoch 6200 \t train loss 0.39016849\n",
      "epoch 6250 \t train loss 0.38830372\n",
      "epoch 6300 \t train loss 0.38646925\n",
      "epoch 6350 \t train loss 0.38465998\n",
      "epoch 6400 \t train loss 0.38287223\n",
      "epoch 6450 \t train loss 0.38110314\n",
      "epoch 6500 \t train loss 0.37935120\n",
      "epoch 6550 \t train loss 0.37761494\n",
      "epoch 6600 \t train loss 0.37589379\n",
      "epoch 6650 \t train loss 0.37418710\n",
      "epoch 6700 \t train loss 0.37249411\n",
      "epoch 6750 \t train loss 0.37081464\n",
      "epoch 6800 \t train loss 0.36914851\n",
      "epoch 6850 \t train loss 0.36749596\n",
      "epoch 6900 \t train loss 0.36585628\n",
      "epoch 6950 \t train loss 0.36422976\n",
      "epoch 7000 \t train loss 0.36261586\n",
      "epoch 7050 \t train loss 0.36101489\n",
      "epoch 7100 \t train loss 0.35942653\n",
      "epoch 7150 \t train loss 0.35801401\n",
      "epoch 7200 \t train loss 0.35641190\n",
      "epoch 7250 \t train loss 0.35483273\n",
      "epoch 7300 \t train loss 0.35327390\n",
      "epoch 7350 \t train loss 0.35173321\n",
      "epoch 7400 \t train loss 0.35020913\n",
      "epoch 7450 \t train loss 0.34870009\n",
      "epoch 7500 \t train loss 0.34720570\n",
      "epoch 7550 \t train loss 0.34572490\n",
      "epoch 7600 \t train loss 0.34425763\n",
      "epoch 7650 \t train loss 0.34280333\n",
      "epoch 7700 \t train loss 0.34136157\n",
      "epoch 7750 \t train loss 0.34007778\n",
      "epoch 7800 \t train loss 0.33862850\n",
      "epoch 7850 \t train loss 0.33719962\n",
      "epoch 7900 \t train loss 0.33578803\n",
      "epoch 7950 \t train loss 0.33439334\n",
      "epoch 8000 \t train loss 0.33301306\n",
      "epoch 8050 \t train loss 0.33164757\n",
      "epoch 8100 \t train loss 0.33029468\n",
      "epoch 8150 \t train loss 0.32895543\n",
      "epoch 8200 \t train loss 0.32762786\n",
      "epoch 8250 \t train loss 0.32631324\n",
      "epoch 8300 \t train loss 0.32500994\n",
      "epoch 8350 \t train loss 0.32371764\n",
      "epoch 8400 \t train loss 0.32243786\n",
      "epoch 8450 \t train loss 0.32171176\n",
      "epoch 8500 \t train loss 0.32034711\n",
      "epoch 8550 \t train loss 0.31901709\n",
      "epoch 8600 \t train loss 0.31753472\n",
      "epoch 8650 \t train loss 0.31629106\n",
      "epoch 8700 \t train loss 0.31506718\n",
      "epoch 8750 \t train loss 0.31385981\n",
      "epoch 8800 \t train loss 0.31266769\n",
      "epoch 8850 \t train loss 0.31148941\n",
      "epoch 8900 \t train loss 0.31032283\n",
      "epoch 8950 \t train loss 0.30916857\n",
      "epoch 9000 \t train loss 0.30802609\n",
      "epoch 9050 \t train loss 0.30689513\n",
      "epoch 9100 \t train loss 0.30577514\n",
      "epoch 9150 \t train loss 0.30466582\n",
      "epoch 9200 \t train loss 0.30356717\n",
      "epoch 9250 \t train loss 0.30247729\n",
      "epoch 9300 \t train loss 0.30139749\n",
      "epoch 9350 \t train loss 0.30032775\n",
      "epoch 9400 \t train loss 0.29926771\n",
      "epoch 9450 \t train loss 0.29821734\n",
      "epoch 9500 \t train loss 0.29717568\n",
      "epoch 9550 \t train loss 0.29614383\n",
      "epoch 9600 \t train loss 0.29512143\n",
      "epoch 9650 \t train loss 0.29410718\n",
      "epoch 9700 \t train loss 0.29310258\n",
      "epoch 9750 \t train loss 0.29210757\n",
      "epoch 9800 \t train loss 0.29112019\n",
      "epoch 9850 \t train loss 0.29014241\n",
      "epoch 9900 \t train loss 0.28917347\n",
      "epoch 9950 \t train loss 0.28821266\n",
      "epoch 10000 \t train loss 0.28726090\n",
      "epoch 10050 \t train loss 0.28631766\n",
      "epoch 10100 \t train loss 0.28538287\n",
      "epoch 10150 \t train loss 0.28445724\n",
      "epoch 10200 \t train loss 0.28342610\n",
      "epoch 10250 \t train loss 0.28252977\n",
      "epoch 10300 \t train loss 0.28164647\n",
      "epoch 10350 \t train loss 0.28077045\n",
      "epoch 10400 \t train loss 0.27990050\n",
      "epoch 10450 \t train loss 0.27903818\n",
      "epoch 10500 \t train loss 0.27818304\n",
      "epoch 10550 \t train loss 0.27733461\n",
      "epoch 10600 \t train loss 0.27649391\n",
      "epoch 10650 \t train loss 0.27553761\n",
      "epoch 10700 \t train loss 0.27469778\n",
      "epoch 10750 \t train loss 0.27390030\n",
      "epoch 10800 \t train loss 0.27310754\n",
      "epoch 10850 \t train loss 0.27232149\n",
      "epoch 10900 \t train loss 0.27154098\n",
      "epoch 10950 \t train loss 0.27076612\n",
      "epoch 11000 \t train loss 0.26999824\n",
      "epoch 11050 \t train loss 0.26923567\n",
      "epoch 11100 \t train loss 0.26847989\n",
      "epoch 11150 \t train loss 0.26773004\n",
      "epoch 11200 \t train loss 0.26690665\n",
      "epoch 11250 \t train loss 0.26618097\n",
      "epoch 11300 \t train loss 0.26546048\n",
      "epoch 11350 \t train loss 0.26474608\n",
      "epoch 11400 \t train loss 0.26403653\n",
      "epoch 11450 \t train loss 0.26333331\n",
      "epoch 11500 \t train loss 0.26263548\n",
      "epoch 11550 \t train loss 0.26194328\n",
      "epoch 11600 \t train loss 0.26125716\n",
      "epoch 11650 \t train loss 0.26057610\n",
      "epoch 11700 \t train loss 0.25990138\n",
      "epoch 11750 \t train loss 0.25923170\n",
      "epoch 11800 \t train loss 0.25856805\n",
      "epoch 11850 \t train loss 0.25790986\n",
      "epoch 11900 \t train loss 0.25725727\n",
      "epoch 11950 \t train loss 0.25661029\n",
      "epoch 12000 \t train loss 0.25596846\n",
      "epoch 12050 \t train loss 0.25533238\n",
      "epoch 12100 \t train loss 0.25470114\n",
      "epoch 12150 \t train loss 0.25407632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12200 \t train loss 0.25345581\n",
      "epoch 12250 \t train loss 0.25284129\n",
      "epoch 12300 \t train loss 0.25223149\n",
      "epoch 12350 \t train loss 0.25162707\n",
      "epoch 12400 \t train loss 0.25102773\n",
      "epoch 12450 \t train loss 0.25043368\n",
      "epoch 12500 \t train loss 0.24984459\n",
      "epoch 12550 \t train loss 0.24925989\n",
      "epoch 12600 \t train loss 0.24868092\n",
      "epoch 12650 \t train loss 0.24810597\n",
      "epoch 12700 \t train loss 0.24753638\n",
      "epoch 12750 \t train loss 0.24697121\n",
      "epoch 12800 \t train loss 0.24641097\n",
      "epoch 12850 \t train loss 0.24585533\n",
      "epoch 12900 \t train loss 0.24530405\n",
      "epoch 12950 \t train loss 0.24475795\n",
      "epoch 13000 \t train loss 0.24421566\n",
      "epoch 13050 \t train loss 0.24367821\n",
      "epoch 13100 \t train loss 0.24314498\n",
      "epoch 13150 \t train loss 0.24261606\n",
      "epoch 13200 \t train loss 0.24209174\n",
      "epoch 13250 \t train loss 0.24157116\n",
      "epoch 13300 \t train loss 0.24105543\n",
      "epoch 13350 \t train loss 0.24071027\n",
      "epoch 13400 \t train loss 0.24018437\n",
      "epoch 13450 \t train loss 0.23966488\n",
      "epoch 13500 \t train loss 0.23915104\n",
      "epoch 13550 \t train loss 0.23864343\n",
      "epoch 13600 \t train loss 0.23814056\n",
      "epoch 13650 \t train loss 0.23764334\n",
      "epoch 13700 \t train loss 0.23715059\n",
      "epoch 13750 \t train loss 0.23666339\n",
      "epoch 13800 \t train loss 0.23617998\n",
      "epoch 13850 \t train loss 0.23565957\n",
      "epoch 13900 \t train loss 0.23519071\n",
      "epoch 13950 \t train loss 0.23472494\n",
      "epoch 14000 \t train loss 0.23426374\n",
      "epoch 14050 \t train loss 0.23380591\n",
      "epoch 14100 \t train loss 0.23335208\n",
      "epoch 14150 \t train loss 0.23290202\n",
      "epoch 14200 \t train loss 0.23245484\n",
      "epoch 14250 \t train loss 0.23201170\n",
      "epoch 14300 \t train loss 0.23157176\n",
      "epoch 14350 \t train loss 0.23113547\n",
      "epoch 14400 \t train loss 0.23070272\n",
      "epoch 14450 \t train loss 0.23027257\n",
      "epoch 14500 \t train loss 0.22984599\n",
      "epoch 14550 \t train loss 0.22942264\n",
      "epoch 14600 \t train loss 0.22900209\n",
      "epoch 14650 \t train loss 0.22856792\n",
      "epoch 14700 \t train loss 0.22815535\n",
      "epoch 14750 \t train loss 0.22774619\n",
      "epoch 14800 \t train loss 0.22733974\n",
      "epoch 14850 \t train loss 0.22693588\n",
      "epoch 14900 \t train loss 0.22653551\n",
      "epoch 14950 \t train loss 0.22613732\n",
      "epoch 15000 \t train loss 0.22574198\n",
      "epoch 15050 \t train loss 0.22534981\n",
      "epoch 15100 \t train loss 0.22495987\n",
      "epoch 15150 \t train loss 0.22457248\n",
      "epoch 15200 \t train loss 0.22418817\n",
      "epoch 15250 \t train loss 0.22380589\n",
      "epoch 15300 \t train loss 0.22342667\n",
      "epoch 15350 \t train loss 0.22305003\n",
      "epoch 15400 \t train loss 0.22267560\n",
      "epoch 15450 \t train loss 0.22230376\n",
      "epoch 15500 \t train loss 0.22193475\n",
      "epoch 15550 \t train loss 0.22156771\n",
      "epoch 15600 \t train loss 0.22120303\n",
      "epoch 15650 \t train loss 0.22084097\n",
      "epoch 15700 \t train loss 0.22048117\n",
      "epoch 15750 \t train loss 0.22012357\n",
      "epoch 15800 \t train loss 0.21976844\n",
      "epoch 15850 \t train loss 0.21941550\n",
      "epoch 15900 \t train loss 0.21906477\n",
      "epoch 15950 \t train loss 0.21871632\n",
      "epoch 16000 \t train loss 0.21837046\n",
      "epoch 16050 \t train loss 0.21802604\n",
      "epoch 16100 \t train loss 0.21768403\n",
      "epoch 16150 \t train loss 0.21734461\n",
      "epoch 16200 \t train loss 0.21700671\n",
      "epoch 16250 \t train loss 0.21667107\n",
      "epoch 16300 \t train loss 0.21633781\n",
      "epoch 16350 \t train loss 0.21600624\n",
      "epoch 16400 \t train loss 0.21567664\n",
      "epoch 16450 \t train loss 0.21534916\n",
      "epoch 16500 \t train loss 0.21502396\n",
      "epoch 16550 \t train loss 0.21470008\n",
      "epoch 16600 \t train loss 0.21437838\n",
      "epoch 16650 \t train loss 0.21405886\n",
      "epoch 16700 \t train loss 0.21374099\n",
      "epoch 16750 \t train loss 0.21342472\n",
      "epoch 16800 \t train loss 0.21311072\n",
      "epoch 16850 \t train loss 0.21279878\n",
      "epoch 16900 \t train loss 0.21248792\n",
      "epoch 16950 \t train loss 0.21217912\n",
      "epoch 17000 \t train loss 0.21187233\n",
      "epoch 17050 \t train loss 0.21156727\n",
      "epoch 17100 \t train loss 0.21126350\n",
      "epoch 17150 \t train loss 0.21096173\n",
      "epoch 17200 \t train loss 0.21066207\n",
      "epoch 17250 \t train loss 0.21036341\n",
      "epoch 17300 \t train loss 0.21006656\n",
      "epoch 17350 \t train loss 0.20977161\n",
      "epoch 17400 \t train loss 0.20947833\n",
      "epoch 17450 \t train loss 0.20918617\n",
      "epoch 17500 \t train loss 0.20889598\n",
      "epoch 17550 \t train loss 0.20860757\n",
      "epoch 17600 \t train loss 0.20832046\n",
      "epoch 17650 \t train loss 0.20803458\n",
      "epoch 17700 \t train loss 0.20775066\n",
      "epoch 17750 \t train loss 0.20746849\n",
      "epoch 17800 \t train loss 0.20718729\n",
      "epoch 17850 \t train loss 0.20690755\n",
      "epoch 17900 \t train loss 0.20662973\n",
      "epoch 17950 \t train loss 0.20635341\n",
      "epoch 18000 \t train loss 0.20607793\n",
      "epoch 18050 \t train loss 0.20580390\n",
      "epoch 18100 \t train loss 0.20553195\n",
      "epoch 18150 \t train loss 0.20526104\n",
      "epoch 18200 \t train loss 0.20499131\n",
      "epoch 18250 \t train loss 0.20472311\n",
      "epoch 18300 \t train loss 0.20445644\n",
      "epoch 18350 \t train loss 0.20419114\n",
      "epoch 18400 \t train loss 0.20392655\n",
      "epoch 18450 \t train loss 0.20366378\n",
      "epoch 18500 \t train loss 0.20340253\n",
      "epoch 18550 \t train loss 0.20314236\n",
      "epoch 18600 \t train loss 0.20288310\n",
      "epoch 18650 \t train loss 0.20262546\n",
      "epoch 18700 \t train loss 0.20236916\n",
      "epoch 18750 \t train loss 0.20211403\n",
      "epoch 18800 \t train loss 0.20185983\n",
      "epoch 18850 \t train loss 0.20160713\n",
      "epoch 18900 \t train loss 0.20135566\n",
      "epoch 18950 \t train loss 0.20110548\n",
      "epoch 19000 \t train loss 0.20085609\n",
      "epoch 19050 \t train loss 0.20060787\n",
      "epoch 19100 \t train loss 0.20036135\n",
      "epoch 19150 \t train loss 0.20011596\n",
      "epoch 19200 \t train loss 0.19987103\n",
      "epoch 19250 \t train loss 0.19962762\n",
      "epoch 19300 \t train loss 0.19938537\n",
      "epoch 19350 \t train loss 0.19914463\n",
      "epoch 19400 \t train loss 0.19890416\n",
      "epoch 19450 \t train loss 0.19866508\n",
      "epoch 19500 \t train loss 0.19842734\n",
      "epoch 19550 \t train loss 0.19819081\n",
      "epoch 19600 \t train loss 0.19800358\n",
      "epoch 19650 \t train loss 0.19776503\n",
      "epoch 19700 \t train loss 0.19752800\n",
      "epoch 19750 \t train loss 0.19729174\n",
      "epoch 19800 \t train loss 0.19705712\n",
      "epoch 19850 \t train loss 0.19682404\n",
      "epoch 19900 \t train loss 0.19659170\n",
      "epoch 19950 \t train loss 0.19636054\n",
      "epoch 20000 \t train loss 0.19613087\n",
      "epoch 20050 \t train loss 0.19590244\n",
      "epoch 20100 \t train loss 0.19567478\n",
      "epoch 20150 \t train loss 0.19544813\n",
      "epoch 20200 \t train loss 0.19522314\n",
      "epoch 20250 \t train loss 0.19499914\n",
      "epoch 20300 \t train loss 0.19477560\n",
      "epoch 20350 \t train loss 0.19455327\n",
      "epoch 20400 \t train loss 0.19433220\n",
      "epoch 20450 \t train loss 0.19411229\n",
      "epoch 20500 \t train loss 0.19389273\n",
      "epoch 20550 \t train loss 0.19367439\n",
      "epoch 20600 \t train loss 0.19345716\n",
      "epoch 20650 \t train loss 0.19324116\n",
      "epoch 20700 \t train loss 0.19302523\n",
      "epoch 20750 \t train loss 0.19281058\n",
      "epoch 20800 \t train loss 0.19259703\n",
      "epoch 20850 \t train loss 0.19238465\n",
      "epoch 20900 \t train loss 0.19217237\n",
      "epoch 20950 \t train loss 0.19196119\n",
      "epoch 21000 \t train loss 0.19175123\n",
      "epoch 21050 \t train loss 0.19154225\n",
      "epoch 21100 \t train loss 0.19133351\n",
      "epoch 21150 \t train loss 0.19112590\n",
      "epoch 21200 \t train loss 0.19091902\n",
      "epoch 21250 \t train loss 0.19071326\n",
      "epoch 21300 \t train loss 0.19050794\n",
      "epoch 21350 \t train loss 0.19030341\n",
      "epoch 21400 \t train loss 0.19009978\n",
      "epoch 21450 \t train loss 0.18989714\n",
      "epoch 21500 \t train loss 0.18969528\n",
      "epoch 21550 \t train loss 0.18949353\n",
      "epoch 21600 \t train loss 0.18929307\n",
      "epoch 21650 \t train loss 0.18909334\n",
      "epoch 21700 \t train loss 0.18889463\n",
      "epoch 21750 \t train loss 0.18869595\n",
      "epoch 21800 \t train loss 0.18849829\n",
      "epoch 21850 \t train loss 0.18830172\n",
      "epoch 21900 \t train loss 0.18810590\n",
      "epoch 21950 \t train loss 0.18791023\n",
      "epoch 22000 \t train loss 0.18771527\n",
      "epoch 22050 \t train loss 0.18752122\n",
      "epoch 22100 \t train loss 0.18732816\n",
      "epoch 22150 \t train loss 0.18713518\n",
      "epoch 22200 \t train loss 0.18694303\n",
      "epoch 22250 \t train loss 0.18675185\n",
      "epoch 22300 \t train loss 0.18656146\n",
      "epoch 22350 \t train loss 0.18637152\n",
      "epoch 22400 \t train loss 0.18623575\n",
      "epoch 22450 \t train loss 0.18605088\n",
      "epoch 22500 \t train loss 0.18585877\n",
      "epoch 22550 \t train loss 0.18566698\n",
      "epoch 22600 \t train loss 0.18547655\n",
      "epoch 22650 \t train loss 0.18528713\n",
      "epoch 22700 \t train loss 0.18509849\n",
      "epoch 22750 \t train loss 0.18491054\n",
      "epoch 22800 \t train loss 0.18472358\n",
      "epoch 22850 \t train loss 0.18453784\n",
      "epoch 22900 \t train loss 0.18435228\n",
      "epoch 22950 \t train loss 0.18416756\n",
      "epoch 23000 \t train loss 0.18398387\n",
      "epoch 23050 \t train loss 0.18380103\n",
      "epoch 23100 \t train loss 0.18361821\n",
      "epoch 23150 \t train loss 0.18343665\n",
      "epoch 23200 \t train loss 0.18325592\n",
      "epoch 23250 \t train loss 0.18307579\n",
      "epoch 23300 \t train loss 0.18289596\n",
      "epoch 23350 \t train loss 0.18271696\n",
      "epoch 23400 \t train loss 0.18253871\n",
      "epoch 23450 \t train loss 0.18236135\n",
      "epoch 23500 \t train loss 0.18218407\n",
      "epoch 23550 \t train loss 0.18200764\n",
      "epoch 23600 \t train loss 0.18183212\n",
      "epoch 23650 \t train loss 0.18165688\n",
      "epoch 23700 \t train loss 0.18148214\n",
      "epoch 23750 \t train loss 0.18130814\n",
      "epoch 23800 \t train loss 0.18113489\n",
      "epoch 23850 \t train loss 0.18096202\n",
      "epoch 23900 \t train loss 0.18078956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23950 \t train loss 0.18061785\n",
      "epoch 24000 \t train loss 0.18043360\n",
      "epoch 24050 \t train loss 0.18026363\n",
      "epoch 24100 \t train loss 0.18009469\n",
      "epoch 24150 \t train loss 0.17992629\n",
      "epoch 24200 \t train loss 0.17975850\n",
      "epoch 24250 \t train loss 0.17959058\n",
      "epoch 24300 \t train loss 0.17944103\n",
      "epoch 24350 \t train loss 0.17927307\n",
      "epoch 24400 \t train loss 0.17910596\n",
      "epoch 24450 \t train loss 0.17893951\n",
      "epoch 24500 \t train loss 0.17877309\n",
      "epoch 24550 \t train loss 0.17860743\n",
      "epoch 24600 \t train loss 0.17844256\n",
      "epoch 24650 \t train loss 0.17827829\n",
      "epoch 24700 \t train loss 0.17811421\n",
      "epoch 24750 \t train loss 0.17795054\n",
      "epoch 24800 \t train loss 0.17778797\n",
      "epoch 24850 \t train loss 0.17762577\n",
      "epoch 24900 \t train loss 0.17746358\n",
      "epoch 24950 \t train loss 0.17730213\n",
      "epoch 25000 \t train loss 0.17714152\n",
      "epoch 25050 \t train loss 0.17698109\n",
      "epoch 25100 \t train loss 0.17682080\n",
      "epoch 25150 \t train loss 0.17664924\n",
      "epoch 25200 \t train loss 0.17649107\n",
      "epoch 25250 \t train loss 0.17633339\n",
      "epoch 25300 \t train loss 0.17617627\n",
      "epoch 25350 \t train loss 0.17601963\n",
      "epoch 25400 \t train loss 0.17586357\n",
      "epoch 25450 \t train loss 0.17570757\n",
      "epoch 25500 \t train loss 0.17555212\n",
      "epoch 25550 \t train loss 0.17539722\n",
      "epoch 25600 \t train loss 0.17524306\n",
      "epoch 25650 \t train loss 0.17509916\n",
      "epoch 25700 \t train loss 0.17494452\n",
      "epoch 25750 \t train loss 0.17479064\n",
      "epoch 25800 \t train loss 0.17463736\n",
      "epoch 25850 \t train loss 0.17448409\n",
      "epoch 25900 \t train loss 0.17433134\n",
      "epoch 25950 \t train loss 0.17417945\n",
      "epoch 26000 \t train loss 0.17402792\n",
      "epoch 26050 \t train loss 0.17387652\n",
      "epoch 26100 \t train loss 0.17372559\n",
      "epoch 26150 \t train loss 0.17357537\n",
      "epoch 26200 \t train loss 0.17342571\n",
      "epoch 26250 \t train loss 0.17327595\n",
      "epoch 26300 \t train loss 0.17312670\n",
      "epoch 26350 \t train loss 0.17297817\n",
      "epoch 26400 \t train loss 0.17283020\n",
      "epoch 26450 \t train loss 0.17268210\n",
      "epoch 26500 \t train loss 0.17253460\n",
      "epoch 26550 \t train loss 0.17238752\n",
      "epoch 26600 \t train loss 0.17224104\n",
      "epoch 26650 \t train loss 0.17209446\n",
      "epoch 26700 \t train loss 0.17194835\n",
      "epoch 26750 \t train loss 0.17180290\n",
      "epoch 26800 \t train loss 0.17165800\n",
      "epoch 26850 \t train loss 0.17151292\n",
      "epoch 26900 \t train loss 0.17136838\n",
      "epoch 26950 \t train loss 0.17122431\n",
      "epoch 27000 \t train loss 0.17108066\n",
      "epoch 27050 \t train loss 0.17093721\n",
      "epoch 27100 \t train loss 0.17079399\n",
      "epoch 27150 \t train loss 0.17065133\n",
      "epoch 27200 \t train loss 0.17050915\n",
      "epoch 27250 \t train loss 0.17036687\n",
      "epoch 27300 \t train loss 0.17022527\n",
      "epoch 27350 \t train loss 0.17008389\n",
      "epoch 27400 \t train loss 0.16994306\n",
      "epoch 27450 \t train loss 0.16980238\n",
      "epoch 27500 \t train loss 0.16969000\n",
      "epoch 27550 \t train loss 0.16954760\n",
      "epoch 27600 \t train loss 0.16940568\n",
      "epoch 27650 \t train loss 0.16926474\n",
      "epoch 27700 \t train loss 0.16912349\n",
      "epoch 27750 \t train loss 0.16898278\n",
      "epoch 27800 \t train loss 0.16884290\n",
      "epoch 27850 \t train loss 0.16870337\n",
      "epoch 27900 \t train loss 0.16856383\n",
      "epoch 27950 \t train loss 0.16842496\n",
      "epoch 28000 \t train loss 0.16828668\n",
      "epoch 28050 \t train loss 0.16814860\n",
      "epoch 28100 \t train loss 0.16801082\n",
      "epoch 28150 \t train loss 0.16787342\n",
      "epoch 28200 \t train loss 0.16773664\n",
      "epoch 28250 \t train loss 0.16759995\n",
      "epoch 28300 \t train loss 0.16746342\n",
      "epoch 28350 \t train loss 0.16732761\n",
      "epoch 28400 \t train loss 0.16719211\n",
      "epoch 28450 \t train loss 0.16705690\n",
      "epoch 28500 \t train loss 0.16692175\n",
      "epoch 28550 \t train loss 0.16678711\n",
      "epoch 28600 \t train loss 0.16665316\n",
      "epoch 28650 \t train loss 0.16651897\n",
      "epoch 28700 \t train loss 0.16638518\n",
      "epoch 28750 \t train loss 0.16625199\n",
      "epoch 28800 \t train loss 0.16611912\n",
      "epoch 28850 \t train loss 0.16598617\n",
      "epoch 28900 \t train loss 0.16585371\n",
      "epoch 28950 \t train loss 0.16572168\n",
      "epoch 29000 \t train loss 0.16559014\n",
      "epoch 29050 \t train loss 0.16545817\n",
      "epoch 29100 \t train loss 0.16532694\n",
      "epoch 29150 \t train loss 0.16519602\n",
      "epoch 29200 \t train loss 0.16506552\n",
      "epoch 29250 \t train loss 0.16493504\n",
      "epoch 29300 \t train loss 0.16480471\n",
      "epoch 29350 \t train loss 0.16467508\n",
      "epoch 29400 \t train loss 0.16454558\n",
      "epoch 29450 \t train loss 0.16441616\n",
      "epoch 29500 \t train loss 0.16428701\n",
      "epoch 29550 \t train loss 0.16415846\n",
      "epoch 29600 \t train loss 0.16403013\n",
      "epoch 29650 \t train loss 0.16390167\n",
      "epoch 29700 \t train loss 0.16377369\n",
      "epoch 29750 \t train loss 0.16364595\n",
      "epoch 29800 \t train loss 0.16351896\n",
      "epoch 29850 \t train loss 0.16339157\n",
      "epoch 29900 \t train loss 0.16326463\n",
      "epoch 29950 \t train loss 0.16313806\n",
      "epoch 30000 \t train loss 0.16301179\n",
      "epoch 30050 \t train loss 0.16288558\n",
      "epoch 30100 \t train loss 0.16275947\n",
      "epoch 30150 \t train loss 0.16263405\n",
      "epoch 30200 \t train loss 0.16250887\n",
      "epoch 30250 \t train loss 0.16238360\n",
      "epoch 30300 \t train loss 0.16225850\n",
      "epoch 30350 \t train loss 0.16213393\n",
      "epoch 30400 \t train loss 0.16200984\n",
      "epoch 30450 \t train loss 0.16188540\n",
      "epoch 30500 \t train loss 0.16176150\n",
      "epoch 30550 \t train loss 0.16163780\n",
      "epoch 30600 \t train loss 0.16151463\n",
      "epoch 30650 \t train loss 0.16139123\n",
      "epoch 30700 \t train loss 0.16126815\n",
      "epoch 30750 \t train loss 0.16114553\n",
      "epoch 30800 \t train loss 0.16102316\n",
      "epoch 30850 \t train loss 0.16090072\n",
      "epoch 30900 \t train loss 0.16077859\n",
      "epoch 30950 \t train loss 0.16065697\n",
      "epoch 31000 \t train loss 0.16053534\n",
      "epoch 31050 \t train loss 0.16041392\n",
      "epoch 31100 \t train loss 0.16029275\n",
      "epoch 31150 \t train loss 0.16017198\n",
      "epoch 31200 \t train loss 0.16005145\n",
      "epoch 31250 \t train loss 0.15993067\n",
      "epoch 31300 \t train loss 0.15981057\n",
      "epoch 31350 \t train loss 0.15969054\n",
      "epoch 31400 \t train loss 0.15957092\n",
      "epoch 31450 \t train loss 0.15945112\n",
      "epoch 31500 \t train loss 0.15933186\n",
      "epoch 31550 \t train loss 0.15921277\n",
      "epoch 31600 \t train loss 0.15909394\n",
      "epoch 31650 \t train loss 0.15897512\n",
      "epoch 31700 \t train loss 0.15885668\n",
      "epoch 31750 \t train loss 0.15873859\n",
      "epoch 31800 \t train loss 0.15862048\n",
      "epoch 31850 \t train loss 0.15850254\n",
      "epoch 31900 \t train loss 0.15838491\n",
      "epoch 31950 \t train loss 0.15826775\n",
      "epoch 32000 \t train loss 0.15815038\n",
      "epoch 32050 \t train loss 0.15803339\n",
      "epoch 32100 \t train loss 0.15791680\n",
      "epoch 32150 \t train loss 0.15780062\n",
      "epoch 32200 \t train loss 0.15768437\n",
      "epoch 32250 \t train loss 0.15756840\n",
      "epoch 32300 \t train loss 0.15745248\n",
      "epoch 32350 \t train loss 0.15733632\n",
      "epoch 32400 \t train loss 0.15722026\n",
      "epoch 32450 \t train loss 0.15710496\n",
      "epoch 32500 \t train loss 0.15699022\n",
      "epoch 32550 \t train loss 0.15687541\n",
      "epoch 32600 \t train loss 0.15676057\n",
      "epoch 32650 \t train loss 0.15664669\n",
      "epoch 32700 \t train loss 0.15653207\n",
      "epoch 32750 \t train loss 0.15641741\n",
      "epoch 32800 \t train loss 0.15630302\n",
      "epoch 32850 \t train loss 0.15618966\n",
      "epoch 32900 \t train loss 0.15607642\n",
      "epoch 32950 \t train loss 0.15596300\n",
      "epoch 33000 \t train loss 0.15584990\n",
      "epoch 33050 \t train loss 0.15573761\n",
      "epoch 33100 \t train loss 0.15562448\n",
      "epoch 33150 \t train loss 0.15551163\n",
      "epoch 33200 \t train loss 0.15539877\n",
      "epoch 33250 \t train loss 0.15528688\n",
      "epoch 33300 \t train loss 0.15517511\n",
      "epoch 33350 \t train loss 0.15506328\n",
      "epoch 33400 \t train loss 0.15495211\n",
      "epoch 33450 \t train loss 0.15484101\n",
      "epoch 33500 \t train loss 0.15472924\n",
      "epoch 33550 \t train loss 0.15461763\n",
      "epoch 33600 \t train loss 0.15450688\n",
      "epoch 33650 \t train loss 0.15439644\n",
      "epoch 33700 \t train loss 0.15428605\n",
      "epoch 33750 \t train loss 0.15417565\n",
      "epoch 33800 \t train loss 0.15406618\n",
      "epoch 33850 \t train loss 0.15395612\n",
      "epoch 33900 \t train loss 0.15384602\n",
      "epoch 33950 \t train loss 0.15373608\n",
      "epoch 34000 \t train loss 0.15362674\n",
      "epoch 34050 \t train loss 0.15351780\n",
      "epoch 34100 \t train loss 0.15340888\n",
      "epoch 34150 \t train loss 0.15329995\n",
      "epoch 34200 \t train loss 0.15319215\n",
      "epoch 34250 \t train loss 0.15308330\n",
      "epoch 34300 \t train loss 0.15297461\n",
      "epoch 34350 \t train loss 0.15286598\n",
      "epoch 34400 \t train loss 0.15275841\n",
      "epoch 34450 \t train loss 0.15265088\n",
      "epoch 34500 \t train loss 0.15254348\n",
      "epoch 34550 \t train loss 0.15243589\n",
      "epoch 34600 \t train loss 0.15232936\n",
      "epoch 34650 \t train loss 0.15222194\n",
      "epoch 34700 \t train loss 0.15211458\n",
      "epoch 34750 \t train loss 0.15200759\n",
      "epoch 34800 \t train loss 0.15190126\n",
      "epoch 34850 \t train loss 0.15179516\n",
      "epoch 34900 \t train loss 0.15168902\n",
      "epoch 34950 \t train loss 0.15158318\n",
      "epoch 35000 \t train loss 0.15147759\n",
      "epoch 35050 \t train loss 0.15137176\n",
      "epoch 35100 \t train loss 0.15126580\n",
      "epoch 35150 \t train loss 0.15116000\n",
      "epoch 35200 \t train loss 0.15105518\n",
      "epoch 35250 \t train loss 0.15095042\n",
      "epoch 35300 \t train loss 0.15084591\n",
      "epoch 35350 \t train loss 0.15074125\n",
      "epoch 35400 \t train loss 0.15063688\n",
      "epoch 35450 \t train loss 0.15053227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35500 \t train loss 0.15042785\n",
      "epoch 35550 \t train loss 0.15032355\n",
      "epoch 35600 \t train loss 0.15022018\n",
      "epoch 35650 \t train loss 0.15011674\n",
      "epoch 35700 \t train loss 0.15001342\n",
      "epoch 35750 \t train loss 0.14991012\n",
      "epoch 35800 \t train loss 0.14980685\n",
      "epoch 35850 \t train loss 0.14970378\n",
      "epoch 35900 \t train loss 0.14960073\n",
      "epoch 35950 \t train loss 0.14949745\n",
      "epoch 36000 \t train loss 0.14939535\n",
      "epoch 36050 \t train loss 0.14929340\n",
      "epoch 36100 \t train loss 0.14919150\n",
      "epoch 36150 \t train loss 0.14908947\n",
      "epoch 36200 \t train loss 0.14898742\n",
      "epoch 36250 \t train loss 0.14888556\n",
      "epoch 36300 \t train loss 0.14878374\n",
      "epoch 36350 \t train loss 0.14868204\n",
      "epoch 36400 \t train loss 0.14858120\n",
      "epoch 36450 \t train loss 0.14848057\n",
      "epoch 36500 \t train loss 0.14837989\n",
      "epoch 36550 \t train loss 0.14827889\n",
      "epoch 36600 \t train loss 0.14817801\n",
      "epoch 36650 \t train loss 0.14807759\n",
      "epoch 36700 \t train loss 0.14797723\n",
      "epoch 36750 \t train loss 0.14787695\n",
      "epoch 36800 \t train loss 0.14777683\n",
      "epoch 36850 \t train loss 0.14767760\n",
      "epoch 36900 \t train loss 0.14757827\n",
      "epoch 36950 \t train loss 0.14747845\n",
      "epoch 37000 \t train loss 0.14737872\n",
      "epoch 37050 \t train loss 0.14727959\n",
      "epoch 37100 \t train loss 0.14718047\n",
      "epoch 37150 \t train loss 0.14708137\n",
      "epoch 37200 \t train loss 0.14698245\n",
      "epoch 37250 \t train loss 0.14688448\n",
      "epoch 37300 \t train loss 0.14678654\n",
      "epoch 37350 \t train loss 0.14668814\n",
      "epoch 37400 \t train loss 0.14658939\n",
      "epoch 37450 \t train loss 0.14649130\n",
      "epoch 37500 \t train loss 0.14639347\n",
      "epoch 37550 \t train loss 0.14629599\n",
      "epoch 37600 \t train loss 0.14619824\n",
      "epoch 37650 \t train loss 0.14610123\n",
      "epoch 37700 \t train loss 0.14600452\n",
      "epoch 37750 \t train loss 0.14590713\n",
      "epoch 37800 \t train loss 0.14580973\n",
      "epoch 37850 \t train loss 0.14571249\n",
      "epoch 37900 \t train loss 0.14561615\n",
      "epoch 37950 \t train loss 0.14551985\n",
      "epoch 38000 \t train loss 0.14542349\n",
      "epoch 38050 \t train loss 0.14532728\n",
      "epoch 38100 \t train loss 0.14523167\n",
      "epoch 38150 \t train loss 0.14513575\n",
      "epoch 38200 \t train loss 0.14503975\n",
      "epoch 38250 \t train loss 0.14494385\n",
      "epoch 38300 \t train loss 0.14484817\n",
      "epoch 38350 \t train loss 0.14475302\n",
      "epoch 38400 \t train loss 0.14465797\n",
      "epoch 38450 \t train loss 0.14456308\n",
      "epoch 38500 \t train loss 0.14446813\n",
      "epoch 38550 \t train loss 0.14437359\n",
      "epoch 38600 \t train loss 0.14427885\n",
      "epoch 38650 \t train loss 0.14418397\n",
      "epoch 38700 \t train loss 0.14408922\n",
      "epoch 38750 \t train loss 0.14399521\n",
      "epoch 38800 \t train loss 0.14390158\n",
      "epoch 38850 \t train loss 0.14380807\n",
      "epoch 38900 \t train loss 0.14371461\n",
      "epoch 38950 \t train loss 0.14362052\n",
      "epoch 39000 \t train loss 0.14352695\n",
      "epoch 39050 \t train loss 0.14343343\n",
      "epoch 39100 \t train loss 0.14334005\n",
      "epoch 39150 \t train loss 0.14324683\n",
      "epoch 39200 \t train loss 0.14315423\n",
      "epoch 39250 \t train loss 0.14306190\n",
      "epoch 39300 \t train loss 0.14296944\n",
      "epoch 39350 \t train loss 0.14287651\n",
      "epoch 39400 \t train loss 0.14278376\n",
      "epoch 39450 \t train loss 0.14269170\n",
      "epoch 39500 \t train loss 0.14259951\n",
      "epoch 39550 \t train loss 0.14250773\n",
      "epoch 39600 \t train loss 0.14241565\n",
      "epoch 39650 \t train loss 0.14232426\n",
      "epoch 39700 \t train loss 0.14223320\n",
      "epoch 39750 \t train loss 0.14214167\n",
      "epoch 39800 \t train loss 0.14204991\n",
      "epoch 39850 \t train loss 0.14195858\n",
      "epoch 39900 \t train loss 0.14186751\n",
      "epoch 39950 \t train loss 0.14177675\n",
      "epoch 40000 \t train loss 0.14168608\n",
      "epoch 40050 \t train loss 0.14159524\n",
      "epoch 40100 \t train loss 0.14150526\n",
      "epoch 40150 \t train loss 0.14141535\n",
      "epoch 40200 \t train loss 0.14132494\n",
      "epoch 40250 \t train loss 0.14123440\n",
      "epoch 40300 \t train loss 0.14114403\n",
      "epoch 40350 \t train loss 0.14105433\n",
      "epoch 40400 \t train loss 0.14096502\n",
      "epoch 40450 \t train loss 0.14087554\n",
      "epoch 40500 \t train loss 0.14078622\n",
      "epoch 40550 \t train loss 0.14069738\n",
      "epoch 40600 \t train loss 0.14060798\n",
      "epoch 40650 \t train loss 0.14051863\n",
      "epoch 40700 \t train loss 0.14042950\n",
      "epoch 40750 \t train loss 0.14034051\n",
      "epoch 40800 \t train loss 0.14025222\n",
      "epoch 40850 \t train loss 0.14016416\n",
      "epoch 40900 \t train loss 0.14007590\n",
      "epoch 40950 \t train loss 0.13998758\n",
      "epoch 41000 \t train loss 0.13989927\n",
      "epoch 41050 \t train loss 0.13981143\n",
      "epoch 41100 \t train loss 0.13972338\n",
      "epoch 41150 \t train loss 0.13963568\n",
      "epoch 41200 \t train loss 0.13954807\n",
      "epoch 41250 \t train loss 0.13946038\n",
      "epoch 41300 \t train loss 0.13937337\n",
      "epoch 41350 \t train loss 0.13928646\n",
      "epoch 41400 \t train loss 0.13919948\n",
      "epoch 41450 \t train loss 0.13911211\n",
      "epoch 41500 \t train loss 0.13902529\n",
      "epoch 41550 \t train loss 0.13893868\n",
      "epoch 41600 \t train loss 0.13885207\n",
      "epoch 41650 \t train loss 0.13876539\n",
      "epoch 41700 \t train loss 0.13867883\n",
      "epoch 41750 \t train loss 0.13859313\n",
      "epoch 41800 \t train loss 0.13850759\n",
      "epoch 41850 \t train loss 0.13842164\n",
      "epoch 41900 \t train loss 0.13833552\n",
      "epoch 41950 \t train loss 0.13824930\n",
      "epoch 42000 \t train loss 0.13816382\n",
      "epoch 42050 \t train loss 0.13807849\n",
      "epoch 42100 \t train loss 0.13799316\n",
      "epoch 42150 \t train loss 0.13790798\n",
      "epoch 42200 \t train loss 0.13782298\n",
      "epoch 42250 \t train loss 0.13773878\n",
      "epoch 42300 \t train loss 0.13765377\n",
      "epoch 42350 \t train loss 0.13756868\n",
      "epoch 42400 \t train loss 0.13748385\n",
      "epoch 42450 \t train loss 0.13739920\n",
      "epoch 42500 \t train loss 0.13731492\n",
      "epoch 42550 \t train loss 0.13723108\n",
      "epoch 42600 \t train loss 0.13714733\n",
      "epoch 42650 \t train loss 0.13706312\n",
      "epoch 42700 \t train loss 0.13697942\n",
      "epoch 42750 \t train loss 0.13689556\n",
      "epoch 42800 \t train loss 0.13681189\n",
      "epoch 42850 \t train loss 0.13672815\n",
      "epoch 42900 \t train loss 0.13664483\n",
      "epoch 42950 \t train loss 0.13656154\n",
      "epoch 43000 \t train loss 0.13647852\n",
      "epoch 43050 \t train loss 0.13639582\n",
      "epoch 43100 \t train loss 0.13631307\n",
      "epoch 43150 \t train loss 0.13623030\n",
      "epoch 43200 \t train loss 0.13614734\n",
      "epoch 43250 \t train loss 0.13606492\n",
      "epoch 43300 \t train loss 0.13598248\n",
      "epoch 43350 \t train loss 0.13590014\n",
      "epoch 43400 \t train loss 0.13581781\n",
      "epoch 43450 \t train loss 0.13573553\n",
      "epoch 43500 \t train loss 0.13565388\n",
      "epoch 43550 \t train loss 0.13557259\n",
      "epoch 43600 \t train loss 0.13549116\n",
      "epoch 43650 \t train loss 0.13540916\n",
      "epoch 43700 \t train loss 0.13531093\n",
      "epoch 43750 \t train loss 0.13523053\n",
      "epoch 43800 \t train loss 0.13515029\n",
      "epoch 43850 \t train loss 0.13507003\n",
      "epoch 43900 \t train loss 0.13499003\n",
      "epoch 43950 \t train loss 0.13491075\n",
      "epoch 44000 \t train loss 0.13482642\n",
      "epoch 44050 \t train loss 0.13474649\n",
      "epoch 44100 \t train loss 0.13466671\n",
      "epoch 44150 \t train loss 0.13458737\n",
      "epoch 44200 \t train loss 0.13450847\n",
      "epoch 44250 \t train loss 0.13442941\n",
      "epoch 44300 \t train loss 0.13434660\n",
      "epoch 44350 \t train loss 0.13426788\n",
      "epoch 44400 \t train loss 0.13418927\n",
      "epoch 44450 \t train loss 0.13411061\n",
      "epoch 44500 \t train loss 0.13403228\n",
      "epoch 44550 \t train loss 0.13395383\n",
      "epoch 44600 \t train loss 0.13387546\n",
      "epoch 44650 \t train loss 0.13379759\n",
      "epoch 44700 \t train loss 0.13372028\n",
      "epoch 44750 \t train loss 0.13364242\n",
      "epoch 44800 \t train loss 0.13356472\n",
      "epoch 44850 \t train loss 0.13348745\n",
      "epoch 44900 \t train loss 0.13340951\n",
      "epoch 44950 \t train loss 0.13333197\n",
      "epoch 45000 \t train loss 0.13325492\n",
      "epoch 45050 \t train loss 0.13317797\n",
      "epoch 45100 \t train loss 0.13310081\n",
      "epoch 45150 \t train loss 0.13302366\n",
      "epoch 45200 \t train loss 0.13294680\n",
      "epoch 45250 \t train loss 0.13287038\n",
      "epoch 45300 \t train loss 0.13279391\n",
      "epoch 45350 \t train loss 0.13271763\n",
      "epoch 45400 \t train loss 0.13264104\n",
      "epoch 45450 \t train loss 0.13256442\n",
      "epoch 45500 \t train loss 0.13248823\n",
      "epoch 45550 \t train loss 0.13241229\n",
      "epoch 45600 \t train loss 0.13233628\n",
      "epoch 45650 \t train loss 0.13226034\n",
      "epoch 45700 \t train loss 0.13218428\n",
      "epoch 45750 \t train loss 0.13210882\n",
      "epoch 45800 \t train loss 0.13203381\n",
      "epoch 45850 \t train loss 0.13195867\n",
      "epoch 45900 \t train loss 0.13188308\n",
      "epoch 45950 \t train loss 0.13180785\n",
      "epoch 46000 \t train loss 0.13173227\n",
      "epoch 46050 \t train loss 0.13165686\n",
      "epoch 46100 \t train loss 0.13158199\n",
      "epoch 46150 \t train loss 0.13150744\n",
      "epoch 46200 \t train loss 0.13143277\n",
      "epoch 46250 \t train loss 0.13135822\n",
      "epoch 46300 \t train loss 0.13128385\n",
      "epoch 46350 \t train loss 0.13120993\n",
      "epoch 46400 \t train loss 0.13113547\n",
      "epoch 46450 \t train loss 0.13106087\n",
      "epoch 46500 \t train loss 0.13098656\n",
      "epoch 46550 \t train loss 0.13091238\n",
      "epoch 46600 \t train loss 0.13083808\n",
      "epoch 46650 \t train loss 0.13076436\n",
      "epoch 46700 \t train loss 0.13069110\n",
      "epoch 46750 \t train loss 0.13061782\n",
      "epoch 46800 \t train loss 0.13054445\n",
      "epoch 46850 \t train loss 0.13047077\n",
      "epoch 46900 \t train loss 0.13039719\n",
      "epoch 46950 \t train loss 0.13032398\n",
      "epoch 47000 \t train loss 0.13025071\n",
      "epoch 47050 \t train loss 0.13017754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47100 \t train loss 0.13010470\n",
      "epoch 47150 \t train loss 0.13003193\n",
      "epoch 47200 \t train loss 0.12995918\n",
      "epoch 47250 \t train loss 0.12988675\n",
      "epoch 47300 \t train loss 0.12981449\n",
      "epoch 47350 \t train loss 0.12974220\n",
      "epoch 47400 \t train loss 0.12966988\n",
      "epoch 47450 \t train loss 0.12959724\n",
      "epoch 47500 \t train loss 0.12952503\n",
      "epoch 47550 \t train loss 0.12945305\n",
      "epoch 47600 \t train loss 0.12938115\n",
      "epoch 47650 \t train loss 0.12930932\n",
      "epoch 47700 \t train loss 0.12923760\n",
      "epoch 47750 \t train loss 0.12916586\n",
      "epoch 47800 \t train loss 0.12909405\n",
      "epoch 47850 \t train loss 0.12902281\n",
      "epoch 47900 \t train loss 0.12895185\n",
      "epoch 47950 \t train loss 0.12888094\n",
      "epoch 48000 \t train loss 0.12880971\n",
      "epoch 48050 \t train loss 0.12873835\n",
      "epoch 48100 \t train loss 0.12866747\n",
      "epoch 48150 \t train loss 0.12859659\n",
      "epoch 48200 \t train loss 0.12852562\n",
      "epoch 48250 \t train loss 0.12845472\n",
      "epoch 48300 \t train loss 0.12838427\n",
      "epoch 48350 \t train loss 0.12831381\n",
      "epoch 48400 \t train loss 0.12824341\n",
      "epoch 48450 \t train loss 0.12817329\n",
      "epoch 48500 \t train loss 0.12810349\n",
      "epoch 48550 \t train loss 0.12803387\n",
      "epoch 48600 \t train loss 0.12796368\n",
      "epoch 48650 \t train loss 0.12789331\n",
      "epoch 48700 \t train loss 0.12782311\n",
      "epoch 48750 \t train loss 0.12775331\n",
      "epoch 48800 \t train loss 0.12768374\n",
      "epoch 48850 \t train loss 0.12761431\n",
      "epoch 48900 \t train loss 0.12754487\n",
      "epoch 48950 \t train loss 0.12747588\n",
      "epoch 49000 \t train loss 0.12740676\n",
      "epoch 49050 \t train loss 0.12733786\n",
      "epoch 49100 \t train loss 0.12726859\n",
      "epoch 49150 \t train loss 0.12719966\n",
      "epoch 49200 \t train loss 0.12713056\n",
      "epoch 49250 \t train loss 0.12706152\n",
      "epoch 49300 \t train loss 0.12699261\n",
      "epoch 49350 \t train loss 0.12692379\n",
      "epoch 49400 \t train loss 0.12685538\n",
      "epoch 49450 \t train loss 0.12678736\n",
      "epoch 49500 \t train loss 0.12671938\n",
      "epoch 49550 \t train loss 0.12665106\n",
      "epoch 49600 \t train loss 0.12658296\n",
      "epoch 49650 \t train loss 0.12651490\n",
      "epoch 49700 \t train loss 0.12644699\n",
      "epoch 49750 \t train loss 0.12637869\n",
      "epoch 49800 \t train loss 0.12631096\n",
      "epoch 49850 \t train loss 0.12624339\n",
      "epoch 49900 \t train loss 0.12617578\n",
      "epoch 49950 \t train loss 0.12610836\n",
      "epoch 50000 \t train loss 0.12604077\n",
      "epoch 50050 \t train loss 0.12597349\n",
      "epoch 50100 \t train loss 0.12590618\n",
      "epoch 50150 \t train loss 0.12583916\n",
      "epoch 50200 \t train loss 0.12577196\n",
      "epoch 50250 \t train loss 0.12570511\n",
      "epoch 50300 \t train loss 0.12563846\n",
      "epoch 50350 \t train loss 0.12557170\n",
      "epoch 50400 \t train loss 0.12550468\n",
      "epoch 50450 \t train loss 0.12543814\n",
      "epoch 50500 \t train loss 0.12537189\n",
      "epoch 50550 \t train loss 0.12530568\n",
      "epoch 50600 \t train loss 0.12523910\n",
      "epoch 50650 \t train loss 0.12517247\n",
      "epoch 50700 \t train loss 0.12510593\n",
      "epoch 50750 \t train loss 0.12503990\n",
      "epoch 50800 \t train loss 0.12497424\n",
      "epoch 50850 \t train loss 0.12490861\n",
      "epoch 50900 \t train loss 0.12484289\n",
      "epoch 50950 \t train loss 0.12477735\n",
      "epoch 51000 \t train loss 0.12471199\n",
      "epoch 51050 \t train loss 0.12464651\n",
      "epoch 51100 \t train loss 0.12458051\n",
      "epoch 51150 \t train loss 0.12451483\n",
      "epoch 51200 \t train loss 0.12444949\n",
      "epoch 51250 \t train loss 0.12438448\n",
      "epoch 51300 \t train loss 0.12431928\n",
      "epoch 51350 \t train loss 0.12425418\n",
      "epoch 51400 \t train loss 0.12418916\n",
      "epoch 51450 \t train loss 0.12412439\n",
      "epoch 51500 \t train loss 0.12406015\n",
      "epoch 51550 \t train loss 0.12399580\n",
      "epoch 51600 \t train loss 0.12393149\n",
      "epoch 51650 \t train loss 0.12386667\n",
      "epoch 51700 \t train loss 0.12380211\n",
      "epoch 51750 \t train loss 0.12373741\n",
      "epoch 51800 \t train loss 0.12367289\n",
      "epoch 51850 \t train loss 0.12360868\n",
      "epoch 51900 \t train loss 0.12354436\n",
      "epoch 51950 \t train loss 0.12348072\n",
      "epoch 52000 \t train loss 0.12341703\n",
      "epoch 52050 \t train loss 0.12335346\n",
      "epoch 52100 \t train loss 0.12328991\n",
      "epoch 52150 \t train loss 0.12322600\n",
      "epoch 52200 \t train loss 0.12316231\n",
      "epoch 52250 \t train loss 0.12309908\n",
      "epoch 52300 \t train loss 0.12303586\n",
      "epoch 52350 \t train loss 0.12297237\n",
      "epoch 52400 \t train loss 0.12290879\n",
      "epoch 52450 \t train loss 0.12284572\n",
      "epoch 52500 \t train loss 0.12278259\n",
      "epoch 52550 \t train loss 0.12271963\n",
      "epoch 52600 \t train loss 0.12265671\n",
      "epoch 52650 \t train loss 0.12259404\n",
      "epoch 52700 \t train loss 0.12253157\n",
      "epoch 52750 \t train loss 0.12246884\n",
      "epoch 52800 \t train loss 0.12240615\n",
      "epoch 52850 \t train loss 0.12234352\n",
      "epoch 52900 \t train loss 0.12228093\n",
      "epoch 52950 \t train loss 0.12221846\n",
      "epoch 53000 \t train loss 0.12215670\n",
      "epoch 53050 \t train loss 0.12209482\n",
      "epoch 53100 \t train loss 0.12203268\n",
      "epoch 53150 \t train loss 0.12197076\n",
      "epoch 53200 \t train loss 0.12190869\n",
      "epoch 53250 \t train loss 0.12184725\n",
      "epoch 53300 \t train loss 0.12178532\n",
      "epoch 53350 \t train loss 0.12172332\n",
      "epoch 53400 \t train loss 0.12166146\n",
      "epoch 53450 \t train loss 0.12159984\n",
      "epoch 53500 \t train loss 0.12153861\n",
      "epoch 53550 \t train loss 0.12147743\n",
      "epoch 53600 \t train loss 0.12141625\n",
      "epoch 53650 \t train loss 0.12135507\n",
      "epoch 53700 \t train loss 0.12129394\n",
      "epoch 53750 \t train loss 0.12123299\n",
      "epoch 53800 \t train loss 0.12117261\n",
      "epoch 53850 \t train loss 0.12111217\n",
      "epoch 53900 \t train loss 0.12105137\n",
      "epoch 53950 \t train loss 0.12099032\n",
      "epoch 54000 \t train loss 0.12092916\n",
      "epoch 54050 \t train loss 0.12086855\n",
      "epoch 54100 \t train loss 0.12080796\n",
      "epoch 54150 \t train loss 0.12074754\n",
      "epoch 54200 \t train loss 0.12068720\n",
      "epoch 54250 \t train loss 0.12062698\n",
      "epoch 54300 \t train loss 0.12056701\n",
      "epoch 54350 \t train loss 0.12050741\n",
      "epoch 54400 \t train loss 0.12044766\n",
      "epoch 54450 \t train loss 0.12038804\n",
      "epoch 54500 \t train loss 0.12032829\n",
      "epoch 54550 \t train loss 0.12026810\n",
      "epoch 54600 \t train loss 0.12020842\n",
      "epoch 54650 \t train loss 0.12014897\n",
      "epoch 54700 \t train loss 0.12008942\n",
      "epoch 54750 \t train loss 0.12002983\n",
      "epoch 54800 \t train loss 0.11997041\n",
      "epoch 54850 \t train loss 0.11991073\n",
      "epoch 54900 \t train loss 0.11985171\n",
      "epoch 54950 \t train loss 0.11979270\n",
      "epoch 55000 \t train loss 0.11973368\n",
      "epoch 55050 \t train loss 0.11967478\n",
      "epoch 55100 \t train loss 0.11961610\n",
      "epoch 55150 \t train loss 0.11955719\n",
      "epoch 55200 \t train loss 0.11949849\n",
      "epoch 55250 \t train loss 0.11943975\n",
      "epoch 55300 \t train loss 0.11938110\n",
      "epoch 55350 \t train loss 0.11932237\n",
      "epoch 55400 \t train loss 0.11926371\n",
      "epoch 55450 \t train loss 0.11920519\n",
      "epoch 55500 \t train loss 0.11914742\n",
      "epoch 55550 \t train loss 0.11908928\n",
      "epoch 55600 \t train loss 0.11903134\n",
      "epoch 55650 \t train loss 0.11897329\n",
      "epoch 55700 \t train loss 0.11891528\n",
      "epoch 55750 \t train loss 0.11885739\n",
      "epoch 55800 \t train loss 0.11879987\n",
      "epoch 55850 \t train loss 0.11874192\n",
      "epoch 55900 \t train loss 0.11868388\n",
      "epoch 55950 \t train loss 0.11862611\n",
      "epoch 56000 \t train loss 0.11856853\n",
      "epoch 56050 \t train loss 0.11851082\n",
      "epoch 56100 \t train loss 0.11845355\n",
      "epoch 56150 \t train loss 0.11839650\n",
      "epoch 56200 \t train loss 0.11833950\n",
      "epoch 56250 \t train loss 0.11828245\n",
      "epoch 56300 \t train loss 0.11822530\n",
      "epoch 56350 \t train loss 0.11816832\n",
      "epoch 56400 \t train loss 0.11811160\n",
      "epoch 56450 \t train loss 0.11805526\n",
      "epoch 56500 \t train loss 0.11799900\n",
      "epoch 56550 \t train loss 0.11794194\n",
      "epoch 56600 \t train loss 0.11788491\n",
      "epoch 56650 \t train loss 0.11782786\n",
      "epoch 56700 \t train loss 0.11777109\n",
      "epoch 56750 \t train loss 0.11771473\n",
      "epoch 56800 \t train loss 0.11765831\n",
      "epoch 56850 \t train loss 0.11760199\n",
      "epoch 56900 \t train loss 0.11754578\n",
      "epoch 56950 \t train loss 0.11748967\n",
      "epoch 57000 \t train loss 0.11743347\n",
      "epoch 57050 \t train loss 0.11737786\n",
      "epoch 57100 \t train loss 0.11732239\n",
      "epoch 57150 \t train loss 0.11726688\n",
      "epoch 57200 \t train loss 0.11721141\n",
      "epoch 57250 \t train loss 0.11715558\n",
      "epoch 57300 \t train loss 0.11709962\n",
      "epoch 57350 \t train loss 0.11704354\n",
      "epoch 57400 \t train loss 0.11698839\n",
      "epoch 57450 \t train loss 0.11693306\n",
      "epoch 57500 \t train loss 0.11687757\n",
      "epoch 57550 \t train loss 0.11682231\n",
      "epoch 57600 \t train loss 0.11676676\n",
      "epoch 57650 \t train loss 0.11671142\n",
      "epoch 57700 \t train loss 0.11665628\n",
      "epoch 57750 \t train loss 0.11660166\n",
      "epoch 57800 \t train loss 0.11654689\n",
      "epoch 57850 \t train loss 0.11649214\n",
      "epoch 57900 \t train loss 0.11643757\n",
      "epoch 57950 \t train loss 0.11638304\n",
      "epoch 58000 \t train loss 0.11632856\n",
      "epoch 58050 \t train loss 0.11627357\n",
      "epoch 58100 \t train loss 0.11621908\n",
      "epoch 58150 \t train loss 0.11616453\n",
      "epoch 58200 \t train loss 0.11611016\n",
      "epoch 58250 \t train loss 0.11605570\n",
      "epoch 58300 \t train loss 0.11600116\n",
      "epoch 58350 \t train loss 0.11594680\n",
      "epoch 58400 \t train loss 0.11589269\n",
      "epoch 58450 \t train loss 0.11583919\n",
      "epoch 58500 \t train loss 0.11578554\n",
      "epoch 58550 \t train loss 0.11573194\n",
      "epoch 58600 \t train loss 0.11567813\n",
      "epoch 58650 \t train loss 0.11562443\n",
      "epoch 58700 \t train loss 0.11557069\n",
      "epoch 58750 \t train loss 0.11551721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58800 \t train loss 0.11546407\n",
      "epoch 58850 \t train loss 0.11541048\n",
      "epoch 58900 \t train loss 0.11535681\n",
      "epoch 58950 \t train loss 0.11530318\n",
      "epoch 59000 \t train loss 0.11524959\n",
      "epoch 59050 \t train loss 0.11519615\n",
      "epoch 59100 \t train loss 0.11514275\n",
      "epoch 59150 \t train loss 0.11508984\n",
      "epoch 59200 \t train loss 0.11503703\n",
      "epoch 59250 \t train loss 0.11498417\n",
      "epoch 59300 \t train loss 0.11493137\n",
      "epoch 59350 \t train loss 0.11487863\n",
      "epoch 59400 \t train loss 0.11482576\n",
      "epoch 59450 \t train loss 0.11477305\n",
      "epoch 59500 \t train loss 0.11472072\n",
      "epoch 59550 \t train loss 0.11466872\n",
      "epoch 59600 \t train loss 0.11461677\n",
      "epoch 59650 \t train loss 0.11456468\n",
      "epoch 59700 \t train loss 0.11451252\n",
      "epoch 59750 \t train loss 0.11445999\n",
      "epoch 59800 \t train loss 0.11440748\n",
      "epoch 59850 \t train loss 0.11435487\n",
      "epoch 59900 \t train loss 0.11430274\n",
      "epoch 59950 \t train loss 0.11425070\n",
      "epoch 60000 \t train loss 0.11419863\n",
      "epoch 60050 \t train loss 0.11414666\n",
      "epoch 60100 \t train loss 0.11409466\n",
      "epoch 60150 \t train loss 0.11404284\n",
      "epoch 60200 \t train loss 0.11399118\n",
      "epoch 60250 \t train loss 0.11393924\n",
      "epoch 60300 \t train loss 0.11388815\n",
      "epoch 60350 \t train loss 0.11383714\n",
      "epoch 60400 \t train loss 0.11378594\n",
      "epoch 60450 \t train loss 0.11373486\n",
      "epoch 60500 \t train loss 0.11368367\n",
      "epoch 60550 \t train loss 0.11363251\n",
      "epoch 60600 \t train loss 0.11358138\n",
      "epoch 60650 \t train loss 0.11352966\n",
      "epoch 60700 \t train loss 0.11347848\n",
      "epoch 60750 \t train loss 0.11342753\n",
      "epoch 60800 \t train loss 0.11337663\n",
      "epoch 60850 \t train loss 0.11332570\n",
      "epoch 60900 \t train loss 0.11327479\n",
      "epoch 60950 \t train loss 0.11322398\n",
      "epoch 61000 \t train loss 0.11317311\n",
      "epoch 61050 \t train loss 0.11312235\n",
      "epoch 61100 \t train loss 0.11307216\n",
      "epoch 61150 \t train loss 0.11302195\n",
      "epoch 61200 \t train loss 0.11297184\n",
      "epoch 61250 \t train loss 0.11292160\n",
      "epoch 61300 \t train loss 0.11287148\n",
      "epoch 61350 \t train loss 0.11282115\n",
      "epoch 61400 \t train loss 0.11277099\n",
      "epoch 61450 \t train loss 0.11272074\n",
      "epoch 61500 \t train loss 0.11267108\n",
      "epoch 61550 \t train loss 0.11262165\n",
      "epoch 61600 \t train loss 0.11257223\n",
      "epoch 61650 \t train loss 0.11252223\n",
      "epoch 61700 \t train loss 0.11247217\n",
      "epoch 61750 \t train loss 0.11242202\n",
      "epoch 61800 \t train loss 0.11237200\n",
      "epoch 61850 \t train loss 0.11232197\n",
      "epoch 61900 \t train loss 0.11227197\n",
      "epoch 61950 \t train loss 0.11222257\n",
      "epoch 62000 \t train loss 0.11217322\n",
      "epoch 62050 \t train loss 0.11212398\n",
      "epoch 62100 \t train loss 0.11207483\n",
      "epoch 62150 \t train loss 0.11202560\n",
      "epoch 62200 \t train loss 0.11197645\n",
      "epoch 62250 \t train loss 0.11192733\n",
      "epoch 62300 \t train loss 0.11187819\n",
      "epoch 62350 \t train loss 0.11182905\n",
      "epoch 62400 \t train loss 0.11178053\n",
      "epoch 62450 \t train loss 0.11173209\n",
      "epoch 62500 \t train loss 0.11168358\n",
      "epoch 62550 \t train loss 0.11163519\n",
      "epoch 62600 \t train loss 0.11158680\n",
      "epoch 62650 \t train loss 0.11153836\n",
      "epoch 62700 \t train loss 0.11148984\n",
      "epoch 62750 \t train loss 0.11144085\n",
      "epoch 62800 \t train loss 0.11139179\n",
      "epoch 62850 \t train loss 0.11134337\n",
      "epoch 62900 \t train loss 0.11129510\n",
      "epoch 62950 \t train loss 0.11124697\n",
      "epoch 63000 \t train loss 0.11119875\n",
      "epoch 63050 \t train loss 0.11115058\n",
      "epoch 63100 \t train loss 0.11110230\n",
      "epoch 63150 \t train loss 0.11105417\n",
      "epoch 63200 \t train loss 0.11100585\n",
      "epoch 63250 \t train loss 0.11095759\n",
      "epoch 63300 \t train loss 0.11090978\n",
      "epoch 63350 \t train loss 0.11086219\n",
      "epoch 63400 \t train loss 0.11081467\n",
      "epoch 63450 \t train loss 0.11076713\n",
      "epoch 63500 \t train loss 0.11071975\n",
      "epoch 63550 \t train loss 0.11067213\n",
      "epoch 63600 \t train loss 0.11062486\n",
      "epoch 63650 \t train loss 0.11057746\n",
      "epoch 63700 \t train loss 0.11052997\n",
      "epoch 63750 \t train loss 0.11048256\n",
      "epoch 63800 \t train loss 0.11043541\n",
      "epoch 63850 \t train loss 0.11038883\n",
      "epoch 63900 \t train loss 0.11034216\n",
      "epoch 63950 \t train loss 0.11029510\n",
      "epoch 64000 \t train loss 0.11024783\n",
      "epoch 64050 \t train loss 0.11020053\n",
      "epoch 64100 \t train loss 0.11015303\n",
      "epoch 64150 \t train loss 0.11010584\n",
      "epoch 64200 \t train loss 0.11005869\n",
      "epoch 64250 \t train loss 0.11001136\n",
      "epoch 64300 \t train loss 0.10996435\n",
      "epoch 64350 \t train loss 0.10991771\n",
      "epoch 64400 \t train loss 0.10987146\n",
      "epoch 64450 \t train loss 0.10982499\n",
      "epoch 64500 \t train loss 0.10977862\n",
      "epoch 64550 \t train loss 0.10973222\n",
      "epoch 64600 \t train loss 0.10968568\n",
      "epoch 64650 \t train loss 0.10963933\n",
      "epoch 64700 \t train loss 0.10959302\n",
      "epoch 64750 \t train loss 0.10954676\n",
      "epoch 64800 \t train loss 0.10950036\n",
      "epoch 64850 \t train loss 0.10945460\n",
      "epoch 64900 \t train loss 0.10940895\n",
      "epoch 64950 \t train loss 0.10936333\n",
      "epoch 65000 \t train loss 0.10931773\n",
      "epoch 65050 \t train loss 0.10927219\n",
      "epoch 65100 \t train loss 0.10922663\n",
      "epoch 65150 \t train loss 0.10918101\n",
      "epoch 65200 \t train loss 0.10913553\n",
      "epoch 65250 \t train loss 0.10908974\n",
      "epoch 65300 \t train loss 0.10904431\n",
      "epoch 65350 \t train loss 0.10899889\n",
      "epoch 65400 \t train loss 0.10895311\n",
      "epoch 65450 \t train loss 0.10890767\n",
      "epoch 65500 \t train loss 0.10886219\n",
      "epoch 65550 \t train loss 0.10881689\n",
      "epoch 65600 \t train loss 0.10877144\n",
      "epoch 65650 \t train loss 0.10872618\n",
      "epoch 65700 \t train loss 0.10868075\n",
      "epoch 65750 \t train loss 0.10863535\n",
      "epoch 65800 \t train loss 0.10859014\n",
      "epoch 65850 \t train loss 0.10854482\n",
      "epoch 65900 \t train loss 0.10849948\n",
      "epoch 65950 \t train loss 0.10845484\n",
      "epoch 66000 \t train loss 0.10840997\n",
      "epoch 66050 \t train loss 0.10836544\n",
      "epoch 66100 \t train loss 0.10832081\n",
      "epoch 66150 \t train loss 0.10827616\n",
      "epoch 66200 \t train loss 0.10823177\n",
      "epoch 66250 \t train loss 0.10818700\n",
      "epoch 66300 \t train loss 0.10814261\n",
      "epoch 66350 \t train loss 0.10809807\n",
      "epoch 66400 \t train loss 0.10805358\n",
      "epoch 66450 \t train loss 0.10800903\n",
      "epoch 66500 \t train loss 0.10796450\n",
      "epoch 66550 \t train loss 0.10792053\n",
      "epoch 66600 \t train loss 0.10787669\n",
      "epoch 66650 \t train loss 0.10783308\n",
      "epoch 66700 \t train loss 0.10778923\n",
      "epoch 66750 \t train loss 0.10774558\n",
      "epoch 66800 \t train loss 0.10770192\n",
      "epoch 66850 \t train loss 0.10765816\n",
      "epoch 66900 \t train loss 0.10761440\n",
      "epoch 66950 \t train loss 0.10757069\n",
      "epoch 67000 \t train loss 0.10752714\n",
      "epoch 67050 \t train loss 0.10748328\n",
      "epoch 67100 \t train loss 0.10743874\n",
      "epoch 67150 \t train loss 0.10739448\n",
      "epoch 67200 \t train loss 0.10735011\n",
      "epoch 67250 \t train loss 0.10730675\n",
      "epoch 67300 \t train loss 0.10726318\n",
      "epoch 67350 \t train loss 0.10721957\n",
      "epoch 67400 \t train loss 0.10717607\n",
      "epoch 67450 \t train loss 0.10713264\n",
      "epoch 67500 \t train loss 0.10708916\n",
      "epoch 67550 \t train loss 0.10704554\n",
      "epoch 67600 \t train loss 0.10700223\n",
      "epoch 67650 \t train loss 0.10695880\n",
      "epoch 67700 \t train loss 0.10691536\n",
      "epoch 67750 \t train loss 0.10687207\n",
      "epoch 67800 \t train loss 0.10682881\n",
      "epoch 67850 \t train loss 0.10678574\n",
      "epoch 67900 \t train loss 0.10674313\n",
      "epoch 67950 \t train loss 0.10670050\n",
      "epoch 68000 \t train loss 0.10665790\n",
      "epoch 68050 \t train loss 0.10661540\n",
      "epoch 68100 \t train loss 0.10657266\n",
      "epoch 68150 \t train loss 0.10653028\n",
      "epoch 68200 \t train loss 0.10648757\n",
      "epoch 68250 \t train loss 0.10644519\n",
      "epoch 68300 \t train loss 0.10640255\n",
      "epoch 68350 \t train loss 0.10635991\n",
      "epoch 68400 \t train loss 0.10631744\n",
      "epoch 68450 \t train loss 0.10627503\n",
      "epoch 68500 \t train loss 0.10623262\n",
      "epoch 68550 \t train loss 0.10619002\n",
      "epoch 68600 \t train loss 0.10614827\n",
      "epoch 68650 \t train loss 0.10610659\n",
      "epoch 68700 \t train loss 0.10606501\n",
      "epoch 68750 \t train loss 0.10602327\n",
      "epoch 68800 \t train loss 0.10598158\n",
      "epoch 68850 \t train loss 0.10593998\n",
      "epoch 68900 \t train loss 0.10589809\n",
      "epoch 68950 \t train loss 0.10585650\n",
      "epoch 69000 \t train loss 0.10581495\n",
      "epoch 69050 \t train loss 0.10577316\n",
      "epoch 69100 \t train loss 0.10573152\n",
      "epoch 69150 \t train loss 0.10569009\n",
      "epoch 69200 \t train loss 0.10564832\n",
      "epoch 69250 \t train loss 0.10560667\n",
      "epoch 69300 \t train loss 0.10556469\n",
      "epoch 69350 \t train loss 0.10552288\n",
      "epoch 69400 \t train loss 0.10548133\n",
      "epoch 69450 \t train loss 0.10543984\n",
      "epoch 69500 \t train loss 0.10539844\n",
      "epoch 69550 \t train loss 0.10535697\n",
      "epoch 69600 \t train loss 0.10531552\n",
      "epoch 69650 \t train loss 0.10527404\n",
      "epoch 69700 \t train loss 0.10523260\n",
      "epoch 69750 \t train loss 0.10519111\n",
      "epoch 69800 \t train loss 0.10514973\n",
      "epoch 69850 \t train loss 0.10510845\n",
      "epoch 69900 \t train loss 0.10506704\n",
      "epoch 69950 \t train loss 0.10502568\n",
      "epoch 70000 \t train loss 0.10498443\n",
      "epoch 70050 \t train loss 0.10494314\n",
      "epoch 70100 \t train loss 0.10490193\n",
      "epoch 70150 \t train loss 0.10486156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70200 \t train loss 0.10482094\n",
      "epoch 70250 \t train loss 0.10478059\n",
      "epoch 70300 \t train loss 0.10473999\n",
      "epoch 70350 \t train loss 0.10469971\n",
      "epoch 70400 \t train loss 0.10465900\n",
      "epoch 70450 \t train loss 0.10461879\n",
      "epoch 70500 \t train loss 0.10457819\n",
      "epoch 70550 \t train loss 0.10453798\n",
      "epoch 70600 \t train loss 0.10449742\n",
      "epoch 70650 \t train loss 0.10445703\n",
      "epoch 70700 \t train loss 0.10441672\n",
      "epoch 70750 \t train loss 0.10437641\n",
      "epoch 70800 \t train loss 0.10433617\n",
      "epoch 70850 \t train loss 0.10429569\n",
      "epoch 70900 \t train loss 0.10425552\n",
      "epoch 70950 \t train loss 0.10421525\n",
      "epoch 71000 \t train loss 0.10417570\n",
      "epoch 71050 \t train loss 0.10413626\n",
      "epoch 71100 \t train loss 0.10409678\n",
      "epoch 71150 \t train loss 0.10405720\n",
      "epoch 71200 \t train loss 0.10401771\n",
      "epoch 71250 \t train loss 0.10397818\n",
      "epoch 71300 \t train loss 0.10393868\n",
      "epoch 71350 \t train loss 0.10389933\n",
      "epoch 71400 \t train loss 0.10385989\n",
      "epoch 71450 \t train loss 0.10382045\n",
      "epoch 71500 \t train loss 0.10378103\n",
      "epoch 71550 \t train loss 0.10374141\n",
      "epoch 71600 \t train loss 0.10370205\n",
      "epoch 71650 \t train loss 0.10366270\n",
      "epoch 71700 \t train loss 0.10362333\n",
      "epoch 71750 \t train loss 0.10358401\n",
      "epoch 71800 \t train loss 0.10354460\n",
      "epoch 71850 \t train loss 0.10350521\n",
      "epoch 71900 \t train loss 0.10346595\n",
      "epoch 71950 \t train loss 0.10342640\n",
      "epoch 72000 \t train loss 0.10338797\n",
      "epoch 72050 \t train loss 0.10334936\n",
      "epoch 72100 \t train loss 0.10331083\n",
      "epoch 72150 \t train loss 0.10327222\n",
      "epoch 72200 \t train loss 0.10323370\n",
      "epoch 72250 \t train loss 0.10319525\n",
      "epoch 72300 \t train loss 0.10315651\n",
      "epoch 72350 \t train loss 0.10311811\n",
      "epoch 72400 \t train loss 0.10307957\n",
      "epoch 72450 \t train loss 0.10304098\n",
      "epoch 72500 \t train loss 0.10300255\n",
      "epoch 72550 \t train loss 0.10296395\n",
      "epoch 72600 \t train loss 0.10292540\n",
      "epoch 72650 \t train loss 0.10288691\n",
      "epoch 72700 \t train loss 0.10284862\n",
      "epoch 72750 \t train loss 0.10281017\n",
      "epoch 72800 \t train loss 0.10277168\n",
      "epoch 72850 \t train loss 0.10273337\n",
      "epoch 72900 \t train loss 0.10269502\n",
      "epoch 72950 \t train loss 0.10265661\n",
      "epoch 73000 \t train loss 0.10261827\n",
      "epoch 73050 \t train loss 0.10258060\n",
      "epoch 73100 \t train loss 0.10254303\n",
      "epoch 73150 \t train loss 0.10250551\n",
      "epoch 73200 \t train loss 0.10246808\n",
      "epoch 73250 \t train loss 0.10243034\n",
      "epoch 73300 \t train loss 0.10239286\n",
      "epoch 73350 \t train loss 0.10235541\n",
      "epoch 73400 \t train loss 0.10231775\n",
      "epoch 73450 \t train loss 0.10228031\n",
      "epoch 73500 \t train loss 0.10224295\n",
      "epoch 73550 \t train loss 0.10220919\n",
      "epoch 73600 \t train loss 0.10217150\n",
      "epoch 73650 \t train loss 0.10213386\n",
      "epoch 73700 \t train loss 0.10209606\n",
      "epoch 73750 \t train loss 0.10205863\n",
      "epoch 73800 \t train loss 0.10202107\n",
      "epoch 73850 \t train loss 0.10198335\n",
      "epoch 73900 \t train loss 0.10194580\n",
      "epoch 73950 \t train loss 0.10190836\n",
      "epoch 74000 \t train loss 0.10187074\n",
      "epoch 74050 \t train loss 0.10183327\n",
      "epoch 74100 \t train loss 0.10179591\n",
      "epoch 74150 \t train loss 0.10175842\n",
      "epoch 74200 \t train loss 0.10172083\n",
      "epoch 74250 \t train loss 0.10168339\n",
      "epoch 74300 \t train loss 0.10164609\n",
      "epoch 74350 \t train loss 0.10160920\n",
      "epoch 74400 \t train loss 0.10157278\n",
      "epoch 74450 \t train loss 0.10153616\n",
      "epoch 74500 \t train loss 0.10149940\n",
      "epoch 74550 \t train loss 0.10146283\n",
      "epoch 74600 \t train loss 0.10142651\n",
      "epoch 74650 \t train loss 0.10138972\n",
      "epoch 74700 \t train loss 0.10135322\n",
      "epoch 74750 \t train loss 0.10131667\n",
      "epoch 74800 \t train loss 0.10128015\n",
      "epoch 74850 \t train loss 0.10124374\n",
      "epoch 74900 \t train loss 0.10120710\n",
      "epoch 74950 \t train loss 0.10117074\n",
      "epoch 75000 \t train loss 0.10113412\n",
      "epoch 75050 \t train loss 0.10109773\n",
      "epoch 75100 \t train loss 0.10106129\n",
      "epoch 75150 \t train loss 0.10103227\n",
      "epoch 75200 \t train loss 0.10099539\n",
      "epoch 75250 \t train loss 0.10095849\n",
      "epoch 75300 \t train loss 0.10092169\n",
      "epoch 75350 \t train loss 0.10088487\n",
      "epoch 75400 \t train loss 0.10084824\n",
      "epoch 75450 \t train loss 0.10081156\n",
      "epoch 75500 \t train loss 0.10077480\n",
      "epoch 75550 \t train loss 0.10073812\n",
      "epoch 75600 \t train loss 0.10070166\n",
      "epoch 75650 \t train loss 0.10066489\n",
      "epoch 75700 \t train loss 0.10062849\n",
      "epoch 75750 \t train loss 0.10059196\n",
      "epoch 75800 \t train loss 0.10055537\n",
      "epoch 75850 \t train loss 0.10051895\n",
      "epoch 75900 \t train loss 0.10048253\n",
      "epoch 75950 \t train loss 0.10044679\n",
      "epoch 76000 \t train loss 0.10041130\n",
      "epoch 76050 \t train loss 0.10037565\n",
      "epoch 76100 \t train loss 0.10034024\n",
      "epoch 76150 \t train loss 0.10030451\n",
      "epoch 76200 \t train loss 0.10026912\n",
      "epoch 76250 \t train loss 0.10023340\n",
      "epoch 76300 \t train loss 0.10019810\n",
      "epoch 76350 \t train loss 0.10016239\n",
      "epoch 76400 \t train loss 0.10012720\n",
      "epoch 76450 \t train loss 0.10009156\n",
      "epoch 76500 \t train loss 0.10005613\n",
      "epoch 76550 \t train loss 0.10002084\n",
      "epoch 76600 \t train loss 0.09998526\n",
      "epoch 76650 \t train loss 0.09994997\n",
      "epoch 76700 \t train loss 0.09991454\n",
      "epoch 76750 \t train loss 0.09987913\n",
      "epoch 76800 \t train loss 0.09984752\n",
      "epoch 76850 \t train loss 0.09981190\n",
      "epoch 76900 \t train loss 0.09977649\n",
      "epoch 76950 \t train loss 0.09974109\n",
      "epoch 77000 \t train loss 0.09970553\n",
      "epoch 77050 \t train loss 0.09967036\n",
      "epoch 77100 \t train loss 0.09963477\n",
      "epoch 77150 \t train loss 0.09959967\n",
      "epoch 77200 \t train loss 0.09956434\n",
      "epoch 77250 \t train loss 0.09952903\n",
      "epoch 77300 \t train loss 0.09949344\n",
      "epoch 77350 \t train loss 0.09945828\n",
      "epoch 77400 \t train loss 0.09942297\n",
      "epoch 77450 \t train loss 0.09938770\n",
      "epoch 77500 \t train loss 0.09935234\n",
      "epoch 77550 \t train loss 0.09931716\n",
      "epoch 77600 \t train loss 0.09928214\n",
      "epoch 77650 \t train loss 0.09924686\n",
      "epoch 77700 \t train loss 0.09921187\n",
      "epoch 77750 \t train loss 0.09917685\n",
      "epoch 77800 \t train loss 0.09914164\n",
      "epoch 77850 \t train loss 0.09910657\n",
      "epoch 77900 \t train loss 0.09907174\n",
      "epoch 77950 \t train loss 0.09903752\n",
      "epoch 78000 \t train loss 0.09900340\n",
      "epoch 78050 \t train loss 0.09896920\n",
      "epoch 78100 \t train loss 0.09893531\n",
      "epoch 78150 \t train loss 0.09890105\n",
      "epoch 78200 \t train loss 0.09886705\n",
      "epoch 78250 \t train loss 0.09883299\n",
      "epoch 78300 \t train loss 0.09879900\n",
      "epoch 78350 \t train loss 0.09876483\n",
      "epoch 78400 \t train loss 0.09873091\n",
      "epoch 78450 \t train loss 0.09870042\n",
      "epoch 78500 \t train loss 0.09866637\n",
      "epoch 78550 \t train loss 0.09863210\n",
      "epoch 78600 \t train loss 0.09859796\n",
      "epoch 78650 \t train loss 0.09856401\n",
      "epoch 78700 \t train loss 0.09852964\n",
      "epoch 78750 \t train loss 0.09849586\n",
      "epoch 78800 \t train loss 0.09846157\n",
      "epoch 78850 \t train loss 0.09842779\n",
      "epoch 78900 \t train loss 0.09839365\n",
      "epoch 78950 \t train loss 0.09835970\n",
      "epoch 79000 \t train loss 0.09832577\n",
      "epoch 79050 \t train loss 0.09829185\n",
      "epoch 79100 \t train loss 0.09825789\n",
      "epoch 79150 \t train loss 0.09822399\n",
      "epoch 79200 \t train loss 0.09818993\n",
      "epoch 79250 \t train loss 0.09815627\n",
      "epoch 79300 \t train loss 0.09812301\n",
      "epoch 79350 \t train loss 0.09809006\n",
      "epoch 79400 \t train loss 0.09805696\n",
      "epoch 79450 \t train loss 0.09802407\n",
      "epoch 79500 \t train loss 0.09799091\n",
      "epoch 79550 \t train loss 0.09795784\n",
      "epoch 79600 \t train loss 0.09792489\n",
      "epoch 79650 \t train loss 0.09789204\n",
      "epoch 79700 \t train loss 0.09785883\n",
      "epoch 79750 \t train loss 0.09782597\n",
      "epoch 79800 \t train loss 0.09779306\n",
      "epoch 79850 \t train loss 0.09775998\n",
      "epoch 79900 \t train loss 0.09772715\n",
      "epoch 79950 \t train loss 0.09769425\n",
      "epoch 80000 \t train loss 0.09766150\n",
      "epoch 80050 \t train loss 0.09762857\n",
      "epoch 80100 \t train loss 0.09760269\n",
      "epoch 80150 \t train loss 0.09756953\n",
      "epoch 80200 \t train loss 0.09753639\n",
      "epoch 80250 \t train loss 0.09750328\n",
      "epoch 80300 \t train loss 0.09747027\n",
      "epoch 80350 \t train loss 0.09743706\n",
      "epoch 80400 \t train loss 0.09740387\n",
      "epoch 80450 \t train loss 0.09737102\n",
      "epoch 80500 \t train loss 0.09733795\n",
      "epoch 80550 \t train loss 0.09730505\n",
      "epoch 80600 \t train loss 0.09727190\n",
      "epoch 80650 \t train loss 0.09723883\n",
      "epoch 80700 \t train loss 0.09720586\n",
      "epoch 80750 \t train loss 0.09717302\n",
      "epoch 80800 \t train loss 0.09714027\n",
      "epoch 80850 \t train loss 0.09710735\n",
      "epoch 80900 \t train loss 0.09707455\n",
      "epoch 80950 \t train loss 0.09704203\n",
      "epoch 81000 \t train loss 0.09700909\n",
      "epoch 81050 \t train loss 0.09697648\n",
      "epoch 81100 \t train loss 0.09694385\n",
      "epoch 81150 \t train loss 0.09691128\n",
      "epoch 81200 \t train loss 0.09687869\n",
      "epoch 81250 \t train loss 0.09684604\n",
      "epoch 81300 \t train loss 0.09681354\n",
      "epoch 81350 \t train loss 0.09678096\n",
      "epoch 81400 \t train loss 0.09674866\n",
      "epoch 81450 \t train loss 0.09671618\n",
      "epoch 81500 \t train loss 0.09668387\n",
      "epoch 81550 \t train loss 0.09665123\n",
      "epoch 81600 \t train loss 0.09661894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81650 \t train loss 0.09658660\n",
      "epoch 81700 \t train loss 0.09655457\n",
      "epoch 81750 \t train loss 0.09652313\n",
      "epoch 81800 \t train loss 0.09649155\n",
      "epoch 81850 \t train loss 0.09646024\n",
      "epoch 81900 \t train loss 0.09642869\n",
      "epoch 81950 \t train loss 0.09639746\n",
      "epoch 82000 \t train loss 0.09636589\n",
      "epoch 82050 \t train loss 0.09633447\n",
      "epoch 82100 \t train loss 0.09630277\n",
      "epoch 82150 \t train loss 0.09627133\n",
      "epoch 82200 \t train loss 0.09623982\n",
      "epoch 82250 \t train loss 0.09620838\n",
      "epoch 82300 \t train loss 0.09617679\n",
      "epoch 82350 \t train loss 0.09614561\n",
      "epoch 82400 \t train loss 0.09611418\n",
      "epoch 82450 \t train loss 0.09608268\n",
      "epoch 82500 \t train loss 0.09605118\n",
      "epoch 82550 \t train loss 0.09601987\n",
      "epoch 82600 \t train loss 0.09598848\n",
      "epoch 82650 \t train loss 0.09595722\n",
      "epoch 82700 \t train loss 0.09592592\n",
      "epoch 82750 \t train loss 0.09589461\n",
      "epoch 82800 \t train loss 0.09586334\n",
      "epoch 82850 \t train loss 0.09583208\n",
      "epoch 82900 \t train loss 0.09580094\n",
      "epoch 82950 \t train loss 0.09576981\n",
      "epoch 83000 \t train loss 0.09573858\n",
      "epoch 83050 \t train loss 0.09570716\n",
      "epoch 83100 \t train loss 0.09567641\n",
      "epoch 83150 \t train loss 0.09564536\n",
      "epoch 83200 \t train loss 0.09561467\n",
      "epoch 83250 \t train loss 0.09558349\n",
      "epoch 83300 \t train loss 0.09555240\n",
      "epoch 83350 \t train loss 0.09552139\n",
      "epoch 83400 \t train loss 0.09549141\n",
      "epoch 83450 \t train loss 0.09546025\n",
      "epoch 83500 \t train loss 0.09542918\n",
      "epoch 83550 \t train loss 0.09539809\n",
      "epoch 83600 \t train loss 0.09536746\n",
      "epoch 83650 \t train loss 0.09533736\n",
      "epoch 83700 \t train loss 0.09530719\n",
      "epoch 83750 \t train loss 0.09527696\n",
      "epoch 83800 \t train loss 0.09524703\n",
      "epoch 83850 \t train loss 0.09521671\n",
      "epoch 83900 \t train loss 0.09518662\n",
      "epoch 83950 \t train loss 0.09515659\n",
      "epoch 84000 \t train loss 0.09512642\n",
      "epoch 84050 \t train loss 0.09509647\n",
      "epoch 84100 \t train loss 0.09506640\n",
      "epoch 84150 \t train loss 0.09503641\n",
      "epoch 84200 \t train loss 0.09500635\n",
      "epoch 84250 \t train loss 0.09497639\n",
      "epoch 84300 \t train loss 0.09494626\n",
      "epoch 84350 \t train loss 0.09491622\n",
      "epoch 84400 \t train loss 0.09488606\n",
      "epoch 84450 \t train loss 0.09485591\n",
      "epoch 84500 \t train loss 0.09482599\n",
      "epoch 84550 \t train loss 0.09479593\n",
      "epoch 84600 \t train loss 0.09476573\n",
      "epoch 84650 \t train loss 0.09473579\n",
      "epoch 84700 \t train loss 0.09470588\n",
      "epoch 84750 \t train loss 0.09467582\n",
      "epoch 84800 \t train loss 0.09464606\n",
      "epoch 84850 \t train loss 0.09461636\n",
      "epoch 84900 \t train loss 0.09458662\n",
      "epoch 84950 \t train loss 0.09455696\n",
      "epoch 85000 \t train loss 0.09452711\n",
      "epoch 85050 \t train loss 0.09449755\n",
      "epoch 85100 \t train loss 0.09447127\n",
      "epoch 85150 \t train loss 0.09444191\n",
      "epoch 85200 \t train loss 0.09441289\n",
      "epoch 85250 \t train loss 0.09438411\n",
      "epoch 85300 \t train loss 0.09435501\n",
      "epoch 85350 \t train loss 0.09432599\n",
      "epoch 85400 \t train loss 0.09429699\n",
      "epoch 85450 \t train loss 0.09426796\n",
      "epoch 85500 \t train loss 0.09423885\n",
      "epoch 85550 \t train loss 0.09420987\n",
      "epoch 85600 \t train loss 0.09418083\n",
      "epoch 85650 \t train loss 0.09415186\n",
      "epoch 85700 \t train loss 0.09412285\n",
      "epoch 85750 \t train loss 0.09409386\n",
      "epoch 85800 \t train loss 0.09406497\n",
      "epoch 85850 \t train loss 0.09403590\n",
      "epoch 85900 \t train loss 0.09400415\n",
      "epoch 85950 \t train loss 0.09397544\n",
      "epoch 86000 \t train loss 0.09394648\n",
      "epoch 86050 \t train loss 0.09391786\n",
      "epoch 86100 \t train loss 0.09388916\n",
      "epoch 86150 \t train loss 0.09386020\n",
      "epoch 86200 \t train loss 0.09383157\n",
      "epoch 86250 \t train loss 0.09380298\n",
      "epoch 86300 \t train loss 0.09377423\n",
      "epoch 86350 \t train loss 0.09374548\n",
      "epoch 86400 \t train loss 0.09371586\n",
      "epoch 86450 \t train loss 0.09368638\n",
      "epoch 86500 \t train loss 0.09365688\n",
      "epoch 86550 \t train loss 0.09362729\n",
      "epoch 86600 \t train loss 0.09359839\n",
      "epoch 86650 \t train loss 0.09356991\n",
      "epoch 86700 \t train loss 0.09354146\n",
      "epoch 86750 \t train loss 0.09351293\n",
      "epoch 86800 \t train loss 0.09348457\n",
      "epoch 86850 \t train loss 0.09345602\n",
      "epoch 86900 \t train loss 0.09342766\n",
      "epoch 86950 \t train loss 0.09339938\n",
      "epoch 87000 \t train loss 0.09337091\n",
      "epoch 87050 \t train loss 0.09334247\n",
      "epoch 87100 \t train loss 0.09331433\n",
      "epoch 87150 \t train loss 0.09328301\n",
      "epoch 87200 \t train loss 0.09325463\n",
      "epoch 87250 \t train loss 0.09322641\n",
      "epoch 87300 \t train loss 0.09319801\n",
      "epoch 87350 \t train loss 0.09316968\n",
      "epoch 87400 \t train loss 0.09314135\n",
      "epoch 87450 \t train loss 0.09311313\n",
      "epoch 87500 \t train loss 0.09308484\n",
      "epoch 87550 \t train loss 0.09305697\n",
      "epoch 87600 \t train loss 0.09302887\n",
      "epoch 87650 \t train loss 0.09300102\n",
      "epoch 87700 \t train loss 0.09297303\n",
      "epoch 87750 \t train loss 0.09294499\n",
      "epoch 87800 \t train loss 0.09291698\n",
      "epoch 87850 \t train loss 0.09288928\n",
      "epoch 87900 \t train loss 0.09286235\n",
      "epoch 87950 \t train loss 0.09283527\n",
      "epoch 88000 \t train loss 0.09280820\n",
      "epoch 88050 \t train loss 0.09278118\n",
      "epoch 88100 \t train loss 0.09275416\n",
      "epoch 88150 \t train loss 0.09272698\n",
      "epoch 88200 \t train loss 0.09269975\n",
      "epoch 88250 \t train loss 0.09267252\n",
      "epoch 88300 \t train loss 0.09264526\n",
      "epoch 88350 \t train loss 0.09261816\n",
      "epoch 88400 \t train loss 0.09258210\n",
      "epoch 88450 \t train loss 0.09255530\n",
      "epoch 88500 \t train loss 0.09252837\n",
      "epoch 88550 \t train loss 0.09250169\n",
      "epoch 88600 \t train loss 0.09247475\n",
      "epoch 88650 \t train loss 0.09244794\n",
      "epoch 88700 \t train loss 0.09242109\n",
      "epoch 88750 \t train loss 0.09239391\n",
      "epoch 88800 \t train loss 0.09236641\n",
      "epoch 88850 \t train loss 0.09233905\n",
      "epoch 88900 \t train loss 0.09231151\n",
      "epoch 88950 \t train loss 0.09228403\n",
      "epoch 89000 \t train loss 0.09225674\n",
      "epoch 89050 \t train loss 0.09222914\n",
      "epoch 89100 \t train loss 0.09220230\n",
      "epoch 89150 \t train loss 0.09217555\n",
      "epoch 89200 \t train loss 0.09214908\n",
      "epoch 89250 \t train loss 0.09212248\n",
      "epoch 89300 \t train loss 0.09209594\n",
      "epoch 89350 \t train loss 0.09206943\n",
      "epoch 89400 \t train loss 0.09204276\n",
      "epoch 89450 \t train loss 0.09201634\n",
      "epoch 89500 \t train loss 0.09198973\n",
      "epoch 89550 \t train loss 0.09196313\n",
      "epoch 89600 \t train loss 0.09193670\n",
      "epoch 89650 \t train loss 0.09190491\n",
      "epoch 89700 \t train loss 0.09187791\n",
      "epoch 89750 \t train loss 0.09185169\n",
      "epoch 89800 \t train loss 0.09182558\n",
      "epoch 89850 \t train loss 0.09179936\n",
      "epoch 89900 \t train loss 0.09177322\n",
      "epoch 89950 \t train loss 0.09174700\n",
      "epoch 90000 \t train loss 0.09172098\n",
      "epoch 90050 \t train loss 0.09169472\n",
      "epoch 90100 \t train loss 0.09166846\n",
      "epoch 90150 \t train loss 0.09164256\n",
      "epoch 90200 \t train loss 0.09161731\n",
      "epoch 90250 \t train loss 0.09159217\n",
      "epoch 90300 \t train loss 0.09156695\n",
      "epoch 90350 \t train loss 0.09154067\n",
      "epoch 90400 \t train loss 0.09151462\n",
      "epoch 90450 \t train loss 0.09148850\n",
      "epoch 90500 \t train loss 0.09146223\n",
      "epoch 90550 \t train loss 0.09143616\n",
      "epoch 90600 \t train loss 0.09140996\n",
      "epoch 90650 \t train loss 0.09138378\n",
      "epoch 90700 \t train loss 0.09135755\n",
      "epoch 90750 \t train loss 0.09133145\n",
      "epoch 90800 \t train loss 0.09130555\n",
      "epoch 90850 \t train loss 0.09127967\n",
      "epoch 90900 \t train loss 0.09125356\n",
      "epoch 90950 \t train loss 0.09121871\n",
      "epoch 91000 \t train loss 0.09119314\n",
      "epoch 91050 \t train loss 0.09116765\n",
      "epoch 91100 \t train loss 0.09114205\n",
      "epoch 91150 \t train loss 0.09111639\n",
      "epoch 91200 \t train loss 0.09109102\n",
      "epoch 91250 \t train loss 0.09106654\n",
      "epoch 91300 \t train loss 0.09104196\n",
      "epoch 91350 \t train loss 0.09101730\n",
      "epoch 91400 \t train loss 0.09099270\n",
      "epoch 91450 \t train loss 0.09096796\n",
      "epoch 91500 \t train loss 0.09094327\n",
      "epoch 91550 \t train loss 0.09091850\n",
      "epoch 91600 \t train loss 0.09089354\n",
      "epoch 91650 \t train loss 0.09086866\n",
      "epoch 91700 \t train loss 0.09084373\n",
      "epoch 91750 \t train loss 0.09081818\n",
      "epoch 91800 \t train loss 0.09079274\n",
      "epoch 91850 \t train loss 0.09076727\n",
      "epoch 91900 \t train loss 0.09074165\n",
      "epoch 91950 \t train loss 0.09071622\n",
      "epoch 92000 \t train loss 0.09069087\n",
      "epoch 92050 \t train loss 0.09066540\n",
      "epoch 92100 \t train loss 0.09064017\n",
      "epoch 92150 \t train loss 0.09061564\n",
      "epoch 92200 \t train loss 0.09059136\n",
      "epoch 92250 \t train loss 0.09056664\n",
      "epoch 92300 \t train loss 0.09054213\n",
      "epoch 92350 \t train loss 0.09051752\n",
      "epoch 92400 \t train loss 0.09049274\n",
      "epoch 92450 \t train loss 0.09046810\n",
      "epoch 92500 \t train loss 0.09044354\n",
      "epoch 92550 \t train loss 0.09041916\n",
      "epoch 92600 \t train loss 0.09039496\n",
      "epoch 92650 \t train loss 0.09037058\n",
      "epoch 92700 \t train loss 0.09034642\n",
      "epoch 92750 \t train loss 0.09032200\n",
      "epoch 92800 \t train loss 0.09029772\n",
      "epoch 92850 \t train loss 0.09027257\n",
      "epoch 92900 \t train loss 0.09024714\n",
      "epoch 92950 \t train loss 0.09022155\n",
      "epoch 93000 \t train loss 0.09019690\n",
      "epoch 93050 \t train loss 0.09017240\n",
      "epoch 93100 \t train loss 0.09014822\n",
      "epoch 93150 \t train loss 0.09012360\n",
      "epoch 93200 \t train loss 0.09009939\n",
      "epoch 93250 \t train loss 0.09007494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93300 \t train loss 0.09005042\n",
      "epoch 93350 \t train loss 0.09002622\n",
      "epoch 93400 \t train loss 0.09000217\n",
      "epoch 93450 \t train loss 0.08997818\n",
      "epoch 93500 \t train loss 0.08995419\n",
      "epoch 93550 \t train loss 0.08993011\n",
      "epoch 93600 \t train loss 0.08990587\n",
      "epoch 93650 \t train loss 0.08988164\n",
      "epoch 93700 \t train loss 0.08985726\n",
      "epoch 93750 \t train loss 0.08983310\n",
      "epoch 93800 \t train loss 0.08980885\n",
      "epoch 93850 \t train loss 0.08978552\n",
      "epoch 93900 \t train loss 0.08976155\n",
      "epoch 93950 \t train loss 0.08973722\n",
      "epoch 94000 \t train loss 0.08971291\n",
      "epoch 94050 \t train loss 0.08968886\n",
      "epoch 94100 \t train loss 0.08966466\n",
      "epoch 94150 \t train loss 0.08964109\n",
      "epoch 94200 \t train loss 0.08961692\n",
      "epoch 94250 \t train loss 0.08959312\n",
      "epoch 94300 \t train loss 0.08956894\n",
      "epoch 94350 \t train loss 0.08954486\n",
      "epoch 94400 \t train loss 0.08952064\n",
      "epoch 94450 \t train loss 0.08949657\n",
      "epoch 94500 \t train loss 0.08947251\n",
      "epoch 94550 \t train loss 0.08944855\n",
      "epoch 94600 \t train loss 0.08942447\n",
      "epoch 94650 \t train loss 0.08940133\n",
      "epoch 94700 \t train loss 0.08937823\n",
      "epoch 94750 \t train loss 0.08935533\n",
      "epoch 94800 \t train loss 0.08933233\n",
      "epoch 94850 \t train loss 0.08930892\n",
      "epoch 94900 \t train loss 0.08928532\n",
      "epoch 94950 \t train loss 0.08926105\n",
      "epoch 95000 \t train loss 0.08923726\n",
      "epoch 95050 \t train loss 0.08921328\n",
      "epoch 95100 \t train loss 0.08918938\n",
      "epoch 95150 \t train loss 0.08916535\n",
      "epoch 95200 \t train loss 0.08914156\n",
      "epoch 95250 \t train loss 0.08911758\n",
      "epoch 95300 \t train loss 0.08909374\n",
      "epoch 95350 \t train loss 0.08906989\n",
      "epoch 95400 \t train loss 0.08904617\n",
      "epoch 95450 \t train loss 0.08902343\n",
      "epoch 95500 \t train loss 0.08900064\n",
      "epoch 95550 \t train loss 0.08897808\n",
      "epoch 95600 \t train loss 0.08895536\n",
      "epoch 95650 \t train loss 0.08893254\n",
      "epoch 95700 \t train loss 0.08890988\n",
      "epoch 95750 \t train loss 0.08888662\n",
      "epoch 95800 \t train loss 0.08886294\n",
      "epoch 95850 \t train loss 0.08883902\n",
      "epoch 95900 \t train loss 0.08881529\n",
      "epoch 95950 \t train loss 0.08879155\n",
      "epoch 96000 \t train loss 0.08876789\n",
      "epoch 96050 \t train loss 0.08874422\n",
      "epoch 96100 \t train loss 0.08872082\n",
      "epoch 96150 \t train loss 0.08869709\n",
      "epoch 96200 \t train loss 0.08867426\n",
      "epoch 96250 \t train loss 0.08865159\n",
      "epoch 96300 \t train loss 0.08862914\n",
      "epoch 96350 \t train loss 0.08860667\n",
      "epoch 96400 \t train loss 0.08858402\n",
      "epoch 96450 \t train loss 0.08856161\n",
      "epoch 96500 \t train loss 0.08853908\n",
      "epoch 96550 \t train loss 0.08851675\n",
      "epoch 96600 \t train loss 0.08849385\n",
      "epoch 96650 \t train loss 0.08847042\n",
      "epoch 96700 \t train loss 0.08844701\n",
      "epoch 96750 \t train loss 0.08842349\n",
      "epoch 96800 \t train loss 0.08839989\n",
      "epoch 96850 \t train loss 0.08837640\n",
      "epoch 96900 \t train loss 0.08835271\n",
      "epoch 96950 \t train loss 0.08833001\n",
      "epoch 97000 \t train loss 0.08830790\n",
      "epoch 97050 \t train loss 0.08828568\n",
      "epoch 97100 \t train loss 0.08826343\n",
      "epoch 97150 \t train loss 0.08824118\n",
      "epoch 97200 \t train loss 0.08821886\n",
      "epoch 97250 \t train loss 0.08819667\n",
      "epoch 97300 \t train loss 0.08817426\n",
      "epoch 97350 \t train loss 0.08815202\n",
      "epoch 97400 \t train loss 0.08812961\n",
      "epoch 97450 \t train loss 0.08810653\n",
      "epoch 97500 \t train loss 0.08808269\n",
      "epoch 97550 \t train loss 0.08805936\n",
      "epoch 97600 \t train loss 0.08803613\n",
      "epoch 97650 \t train loss 0.08801322\n",
      "epoch 97700 \t train loss 0.08799134\n",
      "epoch 97750 \t train loss 0.08796904\n",
      "epoch 97800 \t train loss 0.08794705\n",
      "epoch 97850 \t train loss 0.08792514\n",
      "epoch 97900 \t train loss 0.08790314\n",
      "epoch 97950 \t train loss 0.08788100\n",
      "epoch 98000 \t train loss 0.08785895\n",
      "epoch 98050 \t train loss 0.08783673\n",
      "epoch 98100 \t train loss 0.08781425\n",
      "epoch 98150 \t train loss 0.08779210\n",
      "epoch 98200 \t train loss 0.08776988\n",
      "epoch 98250 \t train loss 0.08774734\n",
      "epoch 98300 \t train loss 0.08772436\n",
      "epoch 98350 \t train loss 0.08770174\n",
      "epoch 98400 \t train loss 0.08767964\n",
      "epoch 98450 \t train loss 0.08765795\n",
      "epoch 98500 \t train loss 0.08763611\n",
      "epoch 98550 \t train loss 0.08761419\n",
      "epoch 98600 \t train loss 0.08759233\n",
      "epoch 98650 \t train loss 0.08757017\n",
      "epoch 98700 \t train loss 0.08754806\n",
      "epoch 98750 \t train loss 0.08752579\n",
      "epoch 98800 \t train loss 0.08750358\n",
      "epoch 98850 \t train loss 0.08748202\n",
      "epoch 98900 \t train loss 0.08746023\n",
      "epoch 98950 \t train loss 0.08743844\n",
      "epoch 99000 \t train loss 0.08741640\n",
      "epoch 99050 \t train loss 0.08739423\n",
      "epoch 99100 \t train loss 0.08737218\n",
      "epoch 99150 \t train loss 0.08735062\n",
      "epoch 99200 \t train loss 0.08732896\n",
      "epoch 99250 \t train loss 0.08730683\n",
      "epoch 99300 \t train loss 0.08728477\n",
      "epoch 99350 \t train loss 0.08726272\n",
      "epoch 99400 \t train loss 0.08724086\n",
      "epoch 99450 \t train loss 0.08721919\n",
      "epoch 99500 \t train loss 0.08719765\n",
      "epoch 99550 \t train loss 0.08717613\n",
      "epoch 99600 \t train loss 0.08715442\n",
      "epoch 99650 \t train loss 0.08713286\n",
      "epoch 99700 \t train loss 0.08711166\n",
      "epoch 99750 \t train loss 0.08709013\n",
      "epoch 99800 \t train loss 0.08706828\n",
      "epoch 99850 \t train loss 0.08704662\n",
      "epoch 99900 \t train loss 0.08702480\n",
      "epoch 99950 \t train loss 0.08700281\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = learn(X_train, Y_train, 100000, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.076\n",
      "Test accuracy: 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.save(\"weights.npy\", np.asarray(weights, dtype=object))\n",
    "np.save(\"biases.npy\", np.asarray(biases, dtype=object))\n",
    "\n",
    "test_out = feed_forward(X_test)\n",
    "test_argmax = np.argmax(test_out, axis=0)\n",
    "Y_test_argmax = np.argmax(Y_test, axis=0)\n",
    "\n",
    "test_losses = cross_entropy_loss.f(Y_test, test_out)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "print(\"Test accuracy: {:.3f}\".format(np.sum(Y_test == test_argmax) / Y_test.shape[1]))\n",
    "\n",
    "samp_loss = sorted(zip(X_test.T, test_losses), key=lambda v: v[1])\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.plot(range(len(accuracies)), accuracies)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
