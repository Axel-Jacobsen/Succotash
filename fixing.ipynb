{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Philosophy of this network:\n",
    "    The goal that I had while writing this was for me to cement my understanding of the basic fully connected feed-forward network.\n",
    "    My original implementation was quite slow, as it was not taking advantage of numpy vectorization - this version does. You can compare\n",
    "    the previous version of this file to this one (filename nn.py, commit 9cb3da3ce582e940ed862f95930879c8be1721d1), and see the \n",
    "    significant training speed differences. I will say, adding vectorization makes the code less readable (and also increases the\n",
    "    required memory) as I had to pad all vectors and matricies with zeros so each set of data had the same shape (and therefore it could be vectorized).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from loss_fcns import squared_loss, cross_entropy_loss\n",
    "from activations import eLU, ReLU, leaky_ReLU, sigmoid, linear, tanh, softmax\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greater_than_9():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    return (\n",
    "        np.load(\"gt9_data/X_train.npy\"),\n",
    "        _get_one_hot(np.load(\"gt9_data/Y_train.npy\"), 2),\n",
    "        np.load(\"gt9_data/X_test.npy\"),\n",
    "        _get_one_hot(np.load(\"gt9_data/Y_test.npy\"), 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def mnist():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    def load_data(fname):\n",
    "        data_folder = \"mnist_data/\"\n",
    "        with open(data_folder + fname, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    x_train = load_data(\"train-images-idx3-ubyte\")\n",
    "    y_train = load_data(\"train-labels-idx1-ubyte\")\n",
    "    x_test = load_data(\"t10k-images-idx3-ubyte\")\n",
    "    y_test = load_data(\"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "    return (\n",
    "        x_train[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_train[8:], 10).reshape((10, -1)),\n",
    "        x_test[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_test[8:], 10).reshape((10, -1)),\n",
    "    )\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "\n",
    "print(\"data loaded\")\n",
    "\n",
    "# net = ffnn.FFNN([784, 512, 128, 10], [tanh, tanh, softmax], cross_entropy_loss)\n",
    "layers = [2, 2, 2]\n",
    "hs = [ReLU, softmax]\n",
    "cost_fcn = cross_entropy_loss\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(random=True):\n",
    "    \"\"\"\n",
    "    random == False for generating empty weight/bias matricies\n",
    "    \"\"\"\n",
    "    layer_arr = layers\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    layer_iter = iter(layer_arr)\n",
    "    prev_dim = layer_iter.__next__()\n",
    "\n",
    "    for i, dim in enumerate(layer_iter):\n",
    "        if random:\n",
    "            bound = np.sqrt(2 / layer_arr[i])\n",
    "            weight_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, prev_dim)).astype(np.float32)\n",
    "            biases_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, 1)).astype(np.float32)\n",
    "        else:\n",
    "            weight_matrix = np.zeros((dim, prev_dim), dtype=np.float32)\n",
    "            biases_matrix = np.zeros((dim, 1), dtype=np.float32)\n",
    "\n",
    "        weights.append(weight_matrix)\n",
    "        biases.append(biases_matrix)\n",
    "        prev_dim = dim\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = make_network()\n",
    "_prev_dW, _prev_db = make_network(random=False)\n",
    "alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) 1.0\n",
      "(2, 10) 10.0\n",
      "(2, 10) 10.000001\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(xs, training=False):\n",
    "    \"\"\"\n",
    "    Feed-forward through the network, saving the activations and non-linearities\n",
    "    after each layer for backprop.\n",
    "\n",
    "    xs has to be of shape (num features, batch_size)\n",
    "    - x, z, a are all vectors of inputs, outputs, and linear outputs at layers\n",
    "    \"\"\"\n",
    "    zs = []\n",
    "    activations = []\n",
    "\n",
    "    activation = xs.astype(np.float32) / 255\n",
    "    activations.append(activation)\n",
    "    for i, (W, b) in enumerate(zip(weights, biases)):\n",
    "        z = np.einsum(\"ij, jb -> ib\", W, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = hs[i].f(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "    return (activation, activations, zs) if training else activation\n",
    "\n",
    "d1 = np.asarray([[1],[1]])\n",
    "print(d1.shape, feed_forward(d1).sum())\n",
    "\n",
    "d2 = np.asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],[9,8,7,6,5,4,3,2,1,0]])\n",
    "print(d2.shape, feed_forward(d2).sum())\n",
    "\n",
    "print(X_train[:,:10].shape, feed_forward(X_train[:,:10]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9],\n",
       "        [3]]),\n",
       " array([[0.38942432],\n",
       "        [0.6105757 ]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def back_prop(xs, ts):\n",
    "    \"\"\"\n",
    "    xs,ts are lists of vectors (ts are targets for training i.e. true output given input x)\n",
    "    \"\"\"\n",
    "    weight_grads, bias_grads = make_network(random=False)\n",
    "    ys, activations, zs = feed_forward(xs, training=True)\n",
    "\n",
    "    # delta_L = grad cost_fcn(outputs) * activation_fcn.deriv(weighted_output_last_layer)\n",
    "    # should be hadamard product\n",
    "    # Also, ys is just activations[-1]\n",
    "    assert ts.shape == ys.shape\n",
    "\n",
    "    delta = cost_fcn.deriv(ts, ys) * hs[-1].deriv(zs[-1])\n",
    "    batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-2])\n",
    "\n",
    "    # sum along batch\n",
    "    bias_grads[-1][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "    weight_grads[-1][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    # back propogate through layers\n",
    "    for l in range(2, len(layers)):\n",
    "        nonlinear_deriv = hs[-l].deriv(zs[-l])\n",
    "        delta = np.dot(weights[-l + 1].T, delta) * nonlinear_deriv\n",
    "        batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-l - 1])\n",
    "\n",
    "        bias_grads[-l][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "        weight_grads[-l][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    for new_b, new_g, self_b, self_g in zip(bias_grads, weight_grads, biases, weights):\n",
    "        assert new_b.shape == self_b.shape\n",
    "        assert new_g.shape == self_g.shape\n",
    "\n",
    "    return weight_grads, bias_grads\n",
    "\n",
    "m = 1\n",
    "n = 10\n",
    "back_prop(X_train[:,m:m+n], Y_train[:,m:m+n]), X_train[:,m:m+n], Y_train[:,m:m+n]\n",
    "X_train[:,m:m+1], feed_forward(X_train[:,m:m+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mini_batch(batch_xs, batch_ys, lr):\n",
    "#     \"\"\"\n",
    "#     batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "#     \"\"\"\n",
    "#     global weights, biases\n",
    "#     weight_grads, bias_grads = back_prop(batch_xs, batch_ys)\n",
    "\n",
    "#     weights = [\n",
    "#         w - lr * weight_grad for w, weight_grad in zip(weights, weight_grads)\n",
    "#     ]\n",
    "#     biases = [b - lr * bias_grad for b, bias_grad in zip(biases, bias_grads)]\n",
    "\n",
    "def mini_batch(batch_xs, batch_ys, lr):\n",
    "    \"\"\"\n",
    "    batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "    \"\"\"\n",
    "    global weights, biases, _prev_dW, _prev_db\n",
    "    weight_grads, bias_grads = back_prop(batch_xs, batch_ys)\n",
    "    print(weight_grads)\n",
    "    _prev_dW = [lr * weight_grad - alpha * prev_dW for weight_grad, pdW in enumerate(weight_grads, _prev_dW)]\n",
    "    _prev_db = [lr * bias_grad - alpha * prev_db for bias_grad, pdb in enumerate(bias_grads, _prev_db)]\n",
    "    weights = [w - dw for dw in self._prev_dW]\n",
    "    biases  = [b - db for db in self._prev_db]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def learn(xs, ys, epochs, batch_size, lr):\n",
    "    \"\"\"\n",
    "    xs/ys is the input/output data, epochs is\n",
    "    the number of batches to train, batch size is the number of input/outputs to\n",
    "    use in each batch, and lr is learning rate.\n",
    "\n",
    "    xs, ys have the input vectors as COLUMNS, so xs shape should be (num_features, batch_size)\n",
    "    e.g. with MNIST, each image is 28*28=784 features, so xs is (784, 60000)\n",
    "    since there are 10 classes in mnist, y should be (10, 60000)\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    # get random indicies from batch\n",
    "    random_indicies = np.random.choice(xs.shape[1], size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        mini_batch(xs[:, random_indicies], ys[:, random_indicies], lr)\n",
    "\n",
    "        ts = ys[:, random_indicies]\n",
    "        ys_out_test = feed_forward(xs[:, random_indicies])\n",
    "\n",
    "        loss = cost_fcn.f(ts, ys_out_test)\n",
    "        train_loss = np.mean(loss)\n",
    "\n",
    "        losses.append(train_loss)\n",
    "        accuracies.append(np.sum(np.argmax(ts, axis=0) == np.argmax(ys_out_test, axis=0)) / ts.shape[-1])\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"epoch {epoch} \\t train loss {train_loss:.8f}\")\n",
    "\n",
    "    return losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.        , 0.        ],\n",
      "       [0.00064441, 0.0002361 ]], dtype=float32), array([[ 0.        , -0.00119653],\n",
      "       [ 0.        ,  0.00119653]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-32888e17dccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-88c339ccc9dd>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(xs, ys, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrandom_indicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_indicies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_indicies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_indicies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bce2e739abd5>\u001b[0m in \u001b[0;36mmini_batch\u001b[0;34m(batch_xs, batch_ys, lr)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mweight_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0m_prev_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_grad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_dW\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mweight_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdW\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_prev_dW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0m_prev_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbias_grad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_db\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbias_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_prev_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_dW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "losses, accuracies = learn(X_train, Y_train, 1000, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.736\n",
      "Test accuracy: 1.000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-25a0c5ea2a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msamp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(\"weights.npy\", np.asarray(weights, dtype=object))\n",
    "np.save(\"biases.npy\", np.asarray(biases, dtype=object))\n",
    "\n",
    "test_out = feed_forward(X_test)\n",
    "test_argmax = np.argmax(test_out, axis=0)\n",
    "Y_test_argmax = np.argmax(Y_test, axis=0)\n",
    "\n",
    "test_losses = cross_entropy_loss.f(Y_test, test_out)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "print(\"Test accuracy: {:.3f}\".format(np.sum(Y_test == test_argmax) / Y_test.shape[1]))\n",
    "\n",
    "samp_loss = sorted(zip(X_test.T, test_losses), key=lambda v: v[1])\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.plot(range(len(accuracies)), accuracies)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = feed_forward(X_test)\n",
    "test_argmax = np.argmax(test_out, axis=0)\n",
    "Y_test_argmax = np.argmax(Y_test, axis=0)\n",
    "\n",
    "test_losses = cross_entropy_loss.f(Y_test, test_out)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "print(\"Test accuracy: {}\".format(np.sum(Y_test_argmax == test_argmax) / Y_test.shape[1]))\n",
    "\n",
    "samp_loss = sorted(zip(X_test.T, test_losses), key=lambda v: v[1])\n",
    "(test_argmax == Y_test_argmax).sum() / Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
