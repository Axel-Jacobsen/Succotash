{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Philosophy of this network:\n",
    "    The goal that I had while writing this was for me to cement my understanding of the basic fully connected feed-forward network.\n",
    "    My original implementation was quite slow, as it was not taking advantage of numpy vectorization - this version does. You can compare\n",
    "    the previous version of this file to this one (filename nn.py, commit 9cb3da3ce582e940ed862f95930879c8be1721d1), and see the \n",
    "    significant training speed differences. I will say, adding vectorization makes the code less readable (and also increases the\n",
    "    required memory) as I had to pad all vectors and matricies with zeros so each set of data had the same shape (and therefore it could be vectorized).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layers, hs, cost_fcn):\n",
    "        assert len(hs) == len(layers) - 1\n",
    "        self.layers = layers\n",
    "        self.hs = hs\n",
    "        self.cost_fcn = cost_fcn\n",
    "        self.max_row, self.max_col = self.get_weights_matrix_max_shape()\n",
    "        self.weights, self.biases = self.make_network()\n",
    "\n",
    "    def make_network(self, random=True):\n",
    "        \"\"\"We padd the weight matricies to the largest weight matrix, so we can vectorize everything and be quick\"\"\"\n",
    "        layer_arr = self.layers\n",
    "        max_row, max_col = self.max_row, self.max_col\n",
    "        assert len(layer_arr) > 2\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        layer_iter = iter(layer_arr)\n",
    "        prev_dim = layer_iter.__next__()\n",
    "\n",
    "        for i, dim in enumerate(layer_iter):\n",
    "            if random:\n",
    "                bound = 4 * np.sqrt(6) / np.sqrt(layer_arr[i] + layer_arr[i + 1])\n",
    "                weight = np.random.uniform(low=-bound, high=bound, size=(dim, prev_dim))\n",
    "                bias = np.random.uniform(low=-bound, high=bound, size=(dim, 1))\n",
    "                padded_weights = self.pad_edges(\n",
    "                    weight, max_row - dim, max_col - prev_dim\n",
    "                )\n",
    "                padded_biases = self.pad_edges(bias, max_row - dim, 0)\n",
    "            else:\n",
    "                padded_weights = np.zeros((max_row, max_col))\n",
    "                padded_biases = np.zeros((max_row, 1))\n",
    "\n",
    "            weights.append(padded_weights)\n",
    "            biases.append(padded_biases)\n",
    "            prev_dim = dim\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def feed_forward(self, xs):\n",
    "        \"\"\"\n",
    "        Feed-forward through the entire network\n",
    "        xs has to be of shape (batch_size, num_rows, 1)\n",
    "        - x, z, a are all vectors of inputs, outputs, and linear outputs at layers\n",
    "        \"\"\"\n",
    "        col_xs = np.copy(xs)\n",
    "\n",
    "        # If xs is not a batch, make it a batch for the ff\n",
    "        if len(col_xs.shape) == 2:\n",
    "            col_xs = col_xs.reshape(1, col_xs.shape[0], col_xs.shape[1])\n",
    "\n",
    "        batch_size, num_rows, _ = col_xs.shape\n",
    "        z = self.pad_edges(col_xs, self.max_row - num_rows, 0)\n",
    "        # make ays and zs a uniform size; that way we can do vectorization\n",
    "        ays = np.zeros((batch_size, self.max_row, len(self.layers) - 1))\n",
    "        zs = np.zeros((batch_size, self.max_row, len(self.layers)))\n",
    "\n",
    "        zs[..., 0] = np.squeeze(z, axis=-1)\n",
    "        for i, W in enumerate(self.weights):\n",
    "            layer_out = zs[..., i, np.newaxis]\n",
    "            a = np.dot(W, layer_out) + self.biases[i]\n",
    "            # shape of a is (batch_size, num_rows, 1) - needs to be (batch_size, num_rows) for this slice of ays\n",
    "            ays[..., i] = np.squeeze(a, axis=-1)\n",
    "            zs[..., i + 1] = np.squeeze(self.hs[i].f(a), axis=-1)\n",
    "\n",
    "        y = zs[..., -1, np.newaxis]\n",
    "        return y, ays, zs\n",
    "\n",
    "    def back_prop(self, xs, ts, batch=False):\n",
    "        \"\"\"\n",
    "        xs,ts are lists of vectors (ts are targets for training i.e. true output given input x)\n",
    "        \"\"\"\n",
    "        grads, biases = self.make_network(random=False)\n",
    "        ys, ays, zs = self.feed_forward(xs)\n",
    "        # delta_L: derivative of Cost fcn w.r.t. zs times derivative of nonlinear fcn of final layer\n",
    "        ts = ts.reshape(-1, self.layers[-1], 1)\n",
    "        ts = self.pad_edges(ts, self.max_row - self.layers[-1], 0)\n",
    "        delta = self.cost_fcn.deriv(ts, ys) * self.hs[-1].deriv(\n",
    "            ays[..., -1, np.newaxis]\n",
    "        )\n",
    "\n",
    "        \"\"\" dC/dw_jk = a_k * d_j \"\"\"\n",
    "        batch_weights = np.einsum(\"bko, bjo -> bjk\", zs[..., -2, np.newaxis], delta)\n",
    "\n",
    "        grads[-1] = np.sum(batch_weights, axis=0)\n",
    "        biases[-1] = np.sum(delta, axis=0)\n",
    "        # back propogate through the layers\n",
    "        for l in range(2, len(self.layers)):\n",
    "            nonlinear_deriv = self.hs[-l].deriv(ays[..., -l, np.newaxis])\n",
    "            delta = (\n",
    "                np.einsum(\"jk, bjo -> bko\", self.weights[-l + 1], delta)\n",
    "                * nonlinear_deriv\n",
    "            )\n",
    "            batch_weights = np.einsum(\n",
    "                \"bko, bjo -> bjk\", zs[..., -l - 1, np.newaxis], delta\n",
    "            )\n",
    "\n",
    "            grads[-l] = np.sum(batch_weights, axis=0)\n",
    "            biases[-l] = np.sum(delta, axis=0)\n",
    "\n",
    "        return grads, biases\n",
    "\n",
    "    def learn(self, xs, ys, epochs, batch_size, lr):\n",
    "        \"\"\"\n",
    "        xs/ys is the input/output data, xs_val/ys_val is input/output validation, epochs is\n",
    "        the number of batches to train, batch size is the number of input/outputs to\n",
    "        use in each batch, and lr is learning rate\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            random_indicies = np.random.choice(xs.shape[0], size=batch_size)\n",
    "            self.mini_batch(xs[random_indicies, :], ys[random_indicies, :], lr)\n",
    "\n",
    "            ys_out_test, _, _ = self.feed_forward(xs)\n",
    "            ys_out_val, _, _ = self.feed_forward(xs_val)\n",
    "\n",
    "            train_loss = np.mean(self.cost_fcn.f(ys, ys_out_test[:, :1]))\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"epoch {epoch} \\t train loss {train_loss:.3f}\")\n",
    "\n",
    "    def mini_batch(self, batch_xs, batch_ys, lr):\n",
    "        \"\"\"\n",
    "        batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "        \"\"\"\n",
    "        weight_grads, bias_grads = self.back_prop(batch_xs, batch_ys)\n",
    "\n",
    "        self.weights = [\n",
    "            w - lr * weight_grad for w, weight_grad in zip(self.weights, weight_grads)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - lr * bias_grad for b, bias_grad in zip(self.biases, bias_grads)\n",
    "        ]\n",
    "\n",
    "    def get_weights_matrix_max_shape(self):\n",
    "        max_row = max_col = 0\n",
    "        for dim, prev_dim in zip(self.layers[1:], self.layers):\n",
    "            max_row, max_col = max(max_row, dim), max(max_col, prev_dim)\n",
    "        return max_row, max_col\n",
    "\n",
    "    def pad_last_2_dims(self, M, bottom_pad, right_pad):\n",
    "        out = [(0, 0) for _ in range(len(M.shape) - 2)]\n",
    "        out.append((0, bottom_pad))\n",
    "        out.append((0, right_pad))\n",
    "        return tuple(out)\n",
    "\n",
    "    def pad_edges(self, M, bottom_pad, right_pad):\n",
    "        pad_tuple = self.pad_last_2_dims(M, bottom_pad, right_pad)\n",
    "        return np.pad(M, pad_tuple, \"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
