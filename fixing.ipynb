{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Philosophy of this network:\n",
    "    The goal that I had while writing this was for me to cement my understanding of the basic fully connected feed-forward network.\n",
    "    My original implementation was quite slow, as it was not taking advantage of numpy vectorization - this version does. You can compare\n",
    "    the previous version of this file to this one (filename nn.py, commit 9cb3da3ce582e940ed862f95930879c8be1721d1), and see the \n",
    "    significant training speed differences. I will say, adding vectorization makes the code less readable (and also increases the\n",
    "    required memory) as I had to pad all vectors and matricies with zeros so each set of data had the same shape (and therefore it could be vectorized).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layers, hs, cost_fcn):\n",
    "        assert len(hs) == len(layers) - 1\n",
    "        self.layers = layers\n",
    "        self.hs = hs\n",
    "        self.cost_fcn = cost_fcn\n",
    "        self.max_row, self.max_col = self.get_weights_matrix_max_shape()\n",
    "        self.weights, self.biases = self.make_network()\n",
    "\n",
    "    def make_network(self, random=True):\n",
    "        \"\"\"We padd the weight matricies to the largest weight matrix, so we can vectorize everything and be quick\"\"\"\n",
    "        layer_arr = self.layers\n",
    "        max_row, max_col = self.max_row, self.max_col\n",
    "        assert len(layer_arr) > 2\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        layer_iter = iter(layer_arr)\n",
    "        prev_dim = layer_iter.__next__()\n",
    "\n",
    "        for i, dim in enumerate(layer_iter):\n",
    "            if random:\n",
    "                bound = 4 * np.sqrt(6) / np.sqrt(layer_arr[i] + layer_arr[i + 1])\n",
    "                weight = np.random.uniform(low=-bound, high=bound, size=(dim, prev_dim))\n",
    "                bias = np.random.uniform(low=-bound, high=bound, size=(dim, 1))\n",
    "                padded_weights = self.pad_edges(\n",
    "                    weight, max_row - dim, max_col - prev_dim\n",
    "                )\n",
    "                padded_biases = self.pad_edges(bias, max_row - dim, 0)\n",
    "            else:\n",
    "                padded_weights = np.zeros((max_row, max_col))\n",
    "                padded_biases = np.zeros((max_row, 1))\n",
    "\n",
    "            weights.append(padded_weights)\n",
    "            biases.append(padded_biases)\n",
    "            prev_dim = dim\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def feed_forward(self, xs):\n",
    "        \"\"\"\n",
    "        Feed-forward through the entire network\n",
    "        xs has to be of shape (batch_size, num_rows, 1)\n",
    "        - x, z, a are all vectors of inputs, outputs, and linear outputs at layers\n",
    "        \"\"\"\n",
    "        col_xs = np.copy(xs)\n",
    "\n",
    "        # If xs is not a batch, make it a batch for the ff\n",
    "        if len(col_xs.shape) == 2:\n",
    "            col_xs = col_xs.reshape(1, col_xs.shape[0], col_xs.shape[1])\n",
    "\n",
    "        batch_size, num_rows, _ = col_xs.shape\n",
    "        z = self.pad_edges(col_xs, self.max_row - num_rows, 0)\n",
    "        # make ays and zs a uniform size; that way we can do vectorization\n",
    "        ays = np.zeros((batch_size, self.max_row, len(self.layers) - 1))\n",
    "        zs = np.zeros((batch_size, self.max_row, len(self.layers)))\n",
    "\n",
    "        zs[..., 0] = np.squeeze(z, axis=-1)\n",
    "        for i, W in enumerate(self.weights):\n",
    "            layer_out = zs[..., i, np.newaxis]\n",
    "            a = np.dot(W, layer_out) + self.biases[i]\n",
    "            # shape of a is (batch_size, num_rows, 1) - needs to be (batch_size, num_rows) for this slice of ays\n",
    "            ays[..., i] = np.squeeze(a, axis=-1)\n",
    "            zs[..., i + 1] = np.squeeze(self.hs[i].f(a), axis=-1)\n",
    "\n",
    "        y = zs[..., -1, np.newaxis]\n",
    "        return y, ays, zs\n",
    "\n",
    "    def back_prop(self, xs, ts, batch=False):\n",
    "        \"\"\"\n",
    "        xs,ts are lists of vectors (ts are targets for training i.e. true output given input x)\n",
    "        \"\"\"\n",
    "        grads, biases = self.make_network(random=False)\n",
    "        ys, ays, zs = self.feed_forward(xs)\n",
    "        # delta_L: derivative of Cost fcn w.r.t. zs times derivative of nonlinear fcn of final layer\n",
    "        ts = ts.reshape(-1, self.layers[-1], 1)\n",
    "        ts = self.pad_edges(ts, self.max_row - self.layers[-1], 0)\n",
    "        delta = self.cost_fcn.deriv(ts, ys) * self.hs[-1].deriv(\n",
    "            ays[..., -1, np.newaxis]\n",
    "        )\n",
    "\n",
    "        \"\"\" dC/dw_jk = a_k * d_j \"\"\"\n",
    "        batch_weights = np.einsum(\"bko, bjo -> bjk\", zs[..., -2, np.newaxis], delta)\n",
    "\n",
    "        grads[-1] = np.sum(batch_weights, axis=0)\n",
    "        biases[-1] = np.sum(delta, axis=0)\n",
    "        # back propogate through the layers\n",
    "        for l in range(2, len(self.layers)):\n",
    "            nonlinear_deriv = self.hs[-l].deriv(ays[..., -l, np.newaxis])\n",
    "            delta = (\n",
    "                np.einsum(\"jk, bjo -> bko\", self.weights[-l + 1], delta)\n",
    "                * nonlinear_deriv\n",
    "            )\n",
    "            batch_weights = np.einsum(\n",
    "                \"bko, bjo -> bjk\", zs[..., -l - 1, np.newaxis], delta\n",
    "            )\n",
    "\n",
    "            grads[-l] = np.sum(batch_weights, axis=0)\n",
    "            biases[-l] = np.sum(delta, axis=0)\n",
    "\n",
    "        return grads, biases\n",
    "\n",
    "    def learn(self, xs, ys, epochs, batch_size, lr):\n",
    "        \"\"\"\n",
    "        xs/ys is the input/output data, xs_val/ys_val is input/output validation, epochs is\n",
    "        the number of batches to train, batch size is the number of input/outputs to\n",
    "        use in each batch, and lr is learning rate\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            random_indicies = np.random.choice(xs.shape[0], size=batch_size)\n",
    "            self.mini_batch(xs[random_indicies, :], ys[random_indicies, :], lr)\n",
    "\n",
    "            ys_out_test, _, _ = self.feed_forward(xs)\n",
    "            ys_out_val, _, _ = self.feed_forward(xs_val)\n",
    "\n",
    "            train_loss = np.mean(self.cost_fcn.f(ys, ys_out_test[:, :1]))\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"epoch {epoch} \\t train loss {train_loss:.3f}\")\n",
    "\n",
    "    def mini_batch(self, batch_xs, batch_ys, lr):\n",
    "        \"\"\"\n",
    "        batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "        \"\"\"\n",
    "        weight_grads, bias_grads = self.back_prop(batch_xs, batch_ys)\n",
    "\n",
    "        self.weights = [\n",
    "            w - lr * weight_grad for w, weight_grad in zip(self.weights, weight_grads)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - lr * bias_grad for b, bias_grad in zip(self.biases, bias_grads)\n",
    "        ]\n",
    "\n",
    "    def get_weights_matrix_max_shape(self):\n",
    "        max_row = max_col = 0\n",
    "        for dim, prev_dim in zip(self.layers[1:], self.layers):\n",
    "            max_row, max_col = max(max_row, dim), max(max_col, prev_dim)\n",
    "        return max_row, max_col\n",
    "\n",
    "    def pad_last_2_dims(self, M, bottom_pad, right_pad):\n",
    "        out = [(0, 0) for _ in range(len(M.shape) - 2)]\n",
    "        out.append((0, bottom_pad))\n",
    "        out.append((0, right_pad))\n",
    "        return tuple(out)\n",
    "\n",
    "    def pad_edges(self, M, bottom_pad, right_pad):\n",
    "        pad_tuple = self.pad_last_2_dims(M, bottom_pad, right_pad)\n",
    "        return np.pad(M, pad_tuple, \"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist():\n",
    "    def load_data(fname):\n",
    "        data_folder = \"mnist_data/\"\n",
    "        with open(data_folder + fname, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    x_train = load_data(\"train-images-idx3-ubyte\")\n",
    "    y_train = load_data(\"train-labels-idx1-ubyte\")\n",
    "    x_test = load_data(\"t10k-images-idx3-ubyte\")\n",
    "    y_test = load_data(\"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "    return (\n",
    "        x_train[16:].reshape((-1, 28, 28)),\n",
    "        _get_one_hot(y_train[8:].reshape((-1, 1)), 10),\n",
    "        x_test[16:].reshape((-1, 28, 28)),\n",
    "        _get_one_hot(y_test[8:].reshape((-1, 1)), 10),\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape((targets.shape[0], nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x11b26aca0>, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 2344\n",
    "\n",
    "imshow(X_train[n]), np.argmax(Y_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
