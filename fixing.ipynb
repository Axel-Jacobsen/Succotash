{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Philosophy of this network:\n",
    "    The goal that I had while writing this was for me to cement my understanding of the basic fully connected feed-forward network.\n",
    "    My original implementation was quite slow, as it was not taking advantage of numpy vectorization - this version does. You can compare\n",
    "    the previous version of this file to this one (filename nn.py, commit 9cb3da3ce582e940ed862f95930879c8be1721d1), and see the \n",
    "    significant training speed differences. I will say, adding vectorization makes the code less readable (and also increases the\n",
    "    required memory) as I had to pad all vectors and matricies with zeros so each set of data had the same shape (and therefore it could be vectorized).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from loss_fcns import squared_loss, cross_entropy_loss\n",
    "from activations import eLU, ReLU, leaky_ReLU, sigmoid, linear, tanh, softmax\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "def greater_than_9():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    return (\n",
    "        np.load(\"fake_data/X_train.npy\"),\n",
    "        _get_one_hot(np.load(\"fake_data/Y_train.npy\"), 2),\n",
    "        np.load(\"fake_data/X_test.npy\"),\n",
    "        _get_one_hot(np.load(\"fake_data/Y_test.npy\"), 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def mnist():\n",
    "    def _get_one_hot(targets, num_classes):\n",
    "        \"\"\"\n",
    "        targets (num_samples,)\n",
    "        output  (num_classes, num_samples)\n",
    "        \"\"\"\n",
    "        ret = np.zeros((num_classes, targets.shape[0]))\n",
    "        ret[targets, np.arange(targets.size)] = 1\n",
    "        return ret\n",
    "\n",
    "    def load_data(fname):\n",
    "        data_folder = \"mnist_data/\"\n",
    "        with open(data_folder + fname, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    x_train = load_data(\"train-images-idx3-ubyte\")\n",
    "    y_train = load_data(\"train-labels-idx1-ubyte\")\n",
    "    x_test = load_data(\"t10k-images-idx3-ubyte\")\n",
    "    y_test = load_data(\"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "    return (\n",
    "        x_train[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_train[8:], 10).reshape((10, -1)),\n",
    "        x_test[16:].reshape((28 * 28, -1), order=\"C\"),\n",
    "        _get_one_hot(y_test[8:], 10).reshape((10, -1)),\n",
    "    )\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = greater_than_9()\n",
    "print(\"data loaded\")\n",
    "\n",
    "# net = ffnn.FFNN([784, 512, 128, 10], [tanh, tanh, softmax], cross_entropy_loss)\n",
    "layers = [2, 2, 2]\n",
    "hs = [ReLU, softmax]\n",
    "cost_fcn = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(random=True):\n",
    "    \"\"\"\n",
    "    random == False for generating empty weight/bias matricies\n",
    "    \"\"\"\n",
    "    layer_arr = layers\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    layer_iter = iter(layer_arr)\n",
    "    prev_dim = layer_iter.__next__()\n",
    "\n",
    "    for i, dim in enumerate(layer_iter):\n",
    "        if random:\n",
    "            bound = np.sqrt(2 / layer_arr[i])\n",
    "            weight_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, prev_dim)).astype(np.float32)\n",
    "            biases_matrix = np.random.uniform(low=-bound, high=bound, size=(dim, 1)).astype(np.float32)\n",
    "        else:\n",
    "            weight_matrix = np.zeros((dim, prev_dim), dtype=np.float32)\n",
    "            biases_matrix = np.zeros((dim, 1), dtype=np.float32)\n",
    "\n",
    "        weights.append(weight_matrix)\n",
    "        biases.append(biases_matrix)\n",
    "        prev_dim = dim\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = make_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) 1.0\n",
      "(2, 10) 10.0\n",
      "(2, 10) 10.000001\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(xs, training=False):\n",
    "    \"\"\"\n",
    "    Feed-forward through the network, saving the activations and non-linearities\n",
    "    after each layer for backprop.\n",
    "\n",
    "    xs has to be of shape (num features, batch_size)\n",
    "    - x, z, a are all vectors of inputs, outputs, and linear outputs at layers\n",
    "    \"\"\"\n",
    "    zs = []\n",
    "    activations = []\n",
    "\n",
    "    activation = xs.astype(np.float32) / 255\n",
    "    activations.append(activation)\n",
    "    for i, (W, b) in enumerate(zip(weights, biases)):\n",
    "        z = np.einsum(\"ij, jb -> ib\", W, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = hs[i].f(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "    return (activation, activations, zs) if training else activation\n",
    "\n",
    "d1 = np.asarray([[1],[1]])\n",
    "print(d1.shape, feed_forward(d1).sum())\n",
    "\n",
    "d2 = np.asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],[9,8,7,6,5,4,3,2,1,0]])\n",
    "print(d2.shape, feed_forward(d2).sum())\n",
    "\n",
    "print(X_train[:,:10].shape, feed_forward(X_train[:,:10]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9],\n",
       "        [3]]),\n",
       " array([[0.38942432],\n",
       "        [0.6105757 ]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def back_prop(xs, ts):\n",
    "    \"\"\"\n",
    "    xs,ts are lists of vectors (ts are targets for training i.e. true output given input x)\n",
    "    \"\"\"\n",
    "    weight_grads, bias_grads = make_network(random=False)\n",
    "    ys, activations, zs = feed_forward(xs, training=True)\n",
    "\n",
    "    # delta_L = grad cost_fcn(outputs) * activation_fcn.deriv(weighted_output_last_layer)\n",
    "    # should be hadamard product\n",
    "    # Also, ys is just activations[-1]\n",
    "    assert ts.shape == ys.shape\n",
    "\n",
    "    delta = cost_fcn.deriv(ts, ys) * hs[-1].deriv(zs[-1])\n",
    "    batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-2])\n",
    "\n",
    "    # sum along batch\n",
    "    bias_grads[-1][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "    weight_grads[-1][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    # back propogate through layers\n",
    "    for l in range(2, len(layers)):\n",
    "        nonlinear_deriv = hs[-l].deriv(zs[-l])\n",
    "        delta = np.dot(weights[-l + 1].T, delta) * nonlinear_deriv\n",
    "        batch_weights = np.einsum(\"ib, jb -> ijb\", delta, activations[-l - 1])\n",
    "\n",
    "        bias_grads[-l][:, :] = np.mean(delta, axis=-1).reshape(-1, 1)\n",
    "        weight_grads[-l][:, :] = np.mean(batch_weights, axis=-1)\n",
    "\n",
    "    for new_b, new_g, self_b, self_g in zip(bias_grads, weight_grads, biases, weights):\n",
    "        assert new_b.shape == self_b.shape\n",
    "        assert new_g.shape == self_g.shape\n",
    "\n",
    "    return weight_grads, bias_grads\n",
    "\n",
    "m = 1\n",
    "n = 10\n",
    "back_prop(X_train[:,m:m+n], Y_train[:,m:m+n]), X_train[:,m:m+n], Y_train[:,m:m+n]\n",
    "X_train[:,m:m+1], feed_forward(X_train[:,m:m+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(batch_xs, batch_ys, lr):\n",
    "    \"\"\"\n",
    "    batch_xs is the batch of inputs, batch_ys is batch of outputs, lr is learning rate\n",
    "    \"\"\"\n",
    "    global weights, biases\n",
    "    weight_grads, bias_grads = back_prop(batch_xs, batch_ys)\n",
    "\n",
    "    weights = [\n",
    "        w - lr * weight_grad for w, weight_grad in zip(weights, weight_grads)\n",
    "    ]\n",
    "    biases = [b - lr * bias_grad for b, bias_grad in zip(biases, bias_grads)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(xs, ys, epochs, batch_size, lr):\n",
    "    \"\"\"\n",
    "    xs/ys is the input/output data, epochs is\n",
    "    the number of batches to train, batch size is the number of input/outputs to\n",
    "    use in each batch, and lr is learning rate.\n",
    "\n",
    "    xs, ys have the input vectors as COLUMNS, so xs shape should be (num_features, batch_size)\n",
    "    e.g. with MNIST, each image is 28*28=784 features, so xs is (784, 60000)\n",
    "    since there are 10 classes in mnist, y should be (10, 60000)\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    # get random indicies from batch\n",
    "    random_indicies = np.random.choice(xs.shape[1], size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        mini_batch(xs[:, random_indicies], ys[:, random_indicies], lr)\n",
    "\n",
    "        ts = ys[:, random_indicies]\n",
    "        ys_out_test = feed_forward(xs[:, random_indicies])\n",
    "\n",
    "        loss = cost_fcn.f(ts, ys_out_test)\n",
    "        train_loss = np.mean(loss)\n",
    "\n",
    "        losses.append(train_loss)\n",
    "        accuracies.append(np.sum(np.argmax(ts, axis=0) == np.argmax(ys_out_test, axis=0)) / ts.shape[-1])\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"epoch {epoch} \\t train loss {train_loss:.8f}\")\n",
    "\n",
    "    return losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t train loss 0.62109479\n",
      "epoch 50 \t train loss 0.63593870\n",
      "epoch 100 \t train loss 0.63679791\n",
      "epoch 150 \t train loss 0.63535124\n",
      "epoch 200 \t train loss 0.63270107\n",
      "epoch 250 \t train loss 0.62969927\n",
      "epoch 300 \t train loss 0.62661826\n",
      "epoch 350 \t train loss 0.62354325\n",
      "epoch 400 \t train loss 0.62049931\n",
      "epoch 450 \t train loss 0.61749308\n",
      "epoch 500 \t train loss 0.61452435\n",
      "epoch 550 \t train loss 0.61159195\n",
      "epoch 600 \t train loss 0.60869382\n",
      "epoch 650 \t train loss 0.60582828\n",
      "epoch 700 \t train loss 0.60299352\n",
      "epoch 750 \t train loss 0.60018831\n",
      "epoch 800 \t train loss 0.59741076\n",
      "epoch 850 \t train loss 0.59465972\n",
      "epoch 900 \t train loss 0.59221759\n",
      "epoch 950 \t train loss 0.58907631\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = learn(X_train, Y_train, 1000, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.606\n",
      "Test accuracy: 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.save(\"weights.npy\", np.asarray(weights, dtype=object))\n",
    "np.save(\"biases.npy\", np.asarray(biases, dtype=object))\n",
    "\n",
    "test_out = feed_forward(X_test)\n",
    "test_argmax = np.argmax(test_out, axis=0)\n",
    "Y_test_argmax = np.argmax(Y_test, axis=0)\n",
    "\n",
    "test_losses = cross_entropy_loss.f(Y_test, test_out)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "print(\"Test accuracy: {:.3f}\".format(np.sum(Y_test == test_argmax) / Y_test.shape[1]))\n",
    "\n",
    "samp_loss = sorted(zip(X_test.T, test_losses), key=lambda v: v[1])\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.plot(range(len(accuracies)), accuracies)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.606\n",
      "Test accuracy: 0.757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = feed_forward(X_test)\n",
    "test_argmax = np.argmax(test_out, axis=0)\n",
    "Y_test_argmax = np.argmax(Y_test, axis=0)\n",
    "\n",
    "test_losses = cross_entropy_loss.f(Y_test, test_out)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "print(\"Test accuracy: {}\".format(np.sum(Y_test_argmax == test_argmax) / Y_test.shape[1]))\n",
    "\n",
    "samp_loss = sorted(zip(X_test.T, test_losses), key=lambda v: v[1])\n",
    "(test_argmax == Y_test_argmax).sum() / Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
